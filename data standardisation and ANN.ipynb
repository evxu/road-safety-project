{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data standardisation\n",
    "1. Encode every features.\n",
    "2. Remove few rows containing missing data\n",
    "3. Create a numpy array (41113*57) including all standarded and prepared data that can feed neural network\n",
    "\n",
    "## Neural network\n",
    "**Data set** \n",
    "    * training set: accident data from the year of 2009-2013, include 29951 samples\n",
    "    * validation set: accident data from the year of 2014, including 5625 samples\n",
    "    * test set: accident data from the year of 2015, including 5537 samples\n",
    "\n",
    "\n",
    "**features**: \n",
    "    number of vehicles, month, week of the year, day of week, sin(time), cos(time), longitude, latitude, speed limit, junction detail, junction control, light conditions, weather conditions, Road surface conditions, special conditions at site, urban or rural area.\n",
    "    \n",
    "**output:**\n",
    "    Accident severity, number of kis\n",
    "\n",
    "\n",
    "1. Build a regression neural network to predict the number of KIS in an accidents.\n",
    "\n",
    "2. Build a classification neural network to classify the accident severity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/XueqiWang/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/Users/XueqiWang/anaconda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import time\n",
    "import math\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "a = '24/7/2016'\n",
    "a = time.strptime(a,\"%d/%m/%Y\")\n",
    "type(a)\n",
    "print time.strftime('%w',a) # Weekday as a decimal number, where 0 is Sunday and 6 is Saturday.\n",
    "print time.strftime('%W',a) # Week number of the year (Monday as the first day of the week) \n",
    "                            # as a decimal number [00,53]. \n",
    "                            # All days in a new year preceding the first Monday are considered to \n",
    "                            # be in week 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment on 2014 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def one_hot_code(encode, column, start, npdata):\n",
    "    rawdata = df[column].values\n",
    "    i = 0\n",
    "    for j in rawdata:\n",
    "        npdata[i, start + encode[j]] = 1\n",
    "        i+=1\n",
    "\n",
    "\n",
    "df = pd.read_csv('data2/Accidents_kis_2014.csv')\n",
    "## select features\n",
    "df = df[['Number_of_Vehicles', 'Date', 'Time','Longitude', \n",
    "         'Latitude', 'Speed_limit', 'Junction_Detail', 'Junction_Control', 'Light_Conditions', \n",
    "         'Weather_Conditions','Road_Surface_Conditions', 'Special_Conditions_at_Site', \n",
    "         'Urban_or_Rural_Area', 'Accident_Severity','Number_of_KIS']]\n",
    "\n",
    "## drop missing data\n",
    "# weather_Conditions\n",
    "# Road_Surface_Conditions\n",
    "df = df.drop(df[df.Road_Surface_Conditions < 0].index)\n",
    "# Special_Conditions_at_site\n",
    "# Urban_or_Rural_Area\n",
    "\n",
    "## Create numpy array\n",
    "npdata = np.zeros(shape=(len(df),57))  # features(0-52)|Accident_severity(53-55)|number_of_kis\n",
    "\n",
    "## number of vehicles\n",
    "npdata[:,0] = df['Number_of_Vehicles'].values\n",
    "\n",
    "## month, week, weekday\n",
    "day = df['Date'].values\n",
    "i = 0\n",
    "for d in day:\n",
    "    d = time.strptime(d, '%d/%m/%Y')\n",
    "    npdata[i,1] = time.strftime('%m', d) # month\n",
    "    npdata[i,2] = time.strftime('%W', d) # week\n",
    "    npdata[i,3] = time.strftime('%w', d) # weekday\n",
    "    i += 1\n",
    "    \n",
    "## time1 and time2 (sin and cos)\n",
    "tt = df['Time'].values\n",
    "i = 0\n",
    "for t in tt:\n",
    "    hm = t.split(':')\n",
    "    t = float(hm[0])*60 + float(hm[1])\n",
    "    npdata[i,4] = math.sin(2* math.pi * t/1440)\n",
    "    npdata[i,5] = math.cos(2* math.pi * t/1440)\n",
    "    \n",
    "## longitude and latitude\n",
    "npdata[:,6] = df['Longitude'].values\n",
    "npdata[:,7] = df['Latitude'].values\n",
    "\n",
    "## speed limit\n",
    "npdata[:,8] = df['Speed_limit'].values\n",
    "\n",
    "## Junction Detail\n",
    "encode = {0: 0, 1:1, 2: 2, 3:3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "one_hot_code(encode = encode, column = 'Junction_Detail', start = 9, npdata = npdata)\n",
    "\n",
    "## Junction Control\n",
    "junc = df['Junction_Control'].values\n",
    "i = 0\n",
    "for j in junc:\n",
    "    if j == -1:\n",
    "        npdata[i,18] = 1\n",
    "    else:\n",
    "        npdata[i, 18+j] = 1\n",
    "    i+=1\n",
    "\n",
    "## Light Conditions\n",
    "encode = {1:0, 4:1, 5:2, 6:3, 7:4}\n",
    "one_hot_code(encode = encode, column = 'Light_Conditions', start = 23, npdata = npdata)\n",
    "\n",
    "## Weather Conditions\n",
    "encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "one_hot_code(encode = encode, column = 'Weather_Conditions', start = 28, npdata = npdata)\n",
    "\n",
    "## Road surface conditions\n",
    "encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6}\n",
    "one_hot_code(encode = encode, column = 'Road_Surface_Conditions', start = 37, npdata = npdata)\n",
    "\n",
    "## Special conditions at site\n",
    "encode = {0:0, 1: 1, 2: 2, 3: 3, 4:4, 5:5, 6:6, 7:7}\n",
    "one_hot_code(encode = encode, column = 'Special_Conditions_at_Site', start = 44, npdata = npdata)\n",
    "\n",
    "## urban or rural\n",
    "ur = df['Urban_or_Rural_Area'].values\n",
    "i = 0\n",
    "for j in ur:\n",
    "    if j==1:\n",
    "        npdata[i, 52] = 1\n",
    "    else:\n",
    "        npdata[i, 52] = -1\n",
    "    i+=1\n",
    "\n",
    "## Accident severity\n",
    "encode = {1:0, 2:1, 3:2}\n",
    "one_hot_code(encode = encode, column = 'Accident_Severity', start = 53, npdata = npdata)\n",
    "\n",
    "## Number of KIS\n",
    "npdata[:,56] = df['Number_of_KIS'].values\n",
    "\n",
    "np.save('data2/demo_2014_numpy_encoded.npy', npdata) # implement encoding, wait for normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npdata = np.load('data2/demo_2014_numpy_encoded.npy')\n",
    "clm = [0, 1, 2, 3,6,7,8]\n",
    "for c in clm:\n",
    "    norm = npdata[:,c]\n",
    "    norm = (norm - np.mean(norm))/np.std(norm)\n",
    "    npdata[:,c] = norm\n",
    "\n",
    "np.save('data2/demo_2014_numpy_standardised.npy', npdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Neural network trial - 2014 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5625, 57)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(npdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 100 took 0.092s\n",
      "  training loss:\t\t0.191360\n",
      "  validation loss:\t\t0.241574\n",
      "Epoch 2 of 100 took 0.091s\n",
      "  training loss:\t\t0.179982\n",
      "  validation loss:\t\t0.239769\n",
      "Epoch 3 of 100 took 0.106s\n",
      "  training loss:\t\t0.177733\n",
      "  validation loss:\t\t0.237417\n",
      "Epoch 4 of 100 took 0.093s\n",
      "  training loss:\t\t0.173950\n",
      "  validation loss:\t\t0.237353\n",
      "Epoch 5 of 100 took 0.091s\n",
      "  training loss:\t\t0.172813\n",
      "  validation loss:\t\t0.243403\n",
      "Epoch 6 of 100 took 0.100s\n",
      "  training loss:\t\t0.169708\n",
      "  validation loss:\t\t0.242203\n",
      "Epoch 7 of 100 took 0.091s\n",
      "  training loss:\t\t0.168837\n",
      "  validation loss:\t\t0.241939\n",
      "Epoch 8 of 100 took 0.091s\n",
      "  training loss:\t\t0.166193\n",
      "  validation loss:\t\t0.254769\n",
      "Epoch 9 of 100 took 0.103s\n",
      "  training loss:\t\t0.165298\n",
      "  validation loss:\t\t0.242451\n",
      "Epoch 10 of 100 took 0.092s\n",
      "  training loss:\t\t0.162526\n",
      "  validation loss:\t\t0.250024\n",
      "Epoch 11 of 100 took 0.092s\n",
      "  training loss:\t\t0.160275\n",
      "  validation loss:\t\t0.243868\n",
      "Epoch 12 of 100 took 0.100s\n",
      "  training loss:\t\t0.158513\n",
      "  validation loss:\t\t0.250748\n",
      "Epoch 13 of 100 took 0.092s\n",
      "  training loss:\t\t0.155472\n",
      "  validation loss:\t\t0.246490\n",
      "Epoch 14 of 100 took 0.090s\n",
      "  training loss:\t\t0.151632\n",
      "  validation loss:\t\t0.255186\n",
      "Epoch 15 of 100 took 0.098s\n",
      "  training loss:\t\t0.149890\n",
      "  validation loss:\t\t0.276309\n",
      "Epoch 16 of 100 took 0.097s\n",
      "  training loss:\t\t0.153191\n",
      "  validation loss:\t\t0.261735\n",
      "Epoch 17 of 100 took 0.091s\n",
      "  training loss:\t\t0.149229\n",
      "  validation loss:\t\t0.252219\n",
      "Epoch 18 of 100 took 0.104s\n",
      "  training loss:\t\t0.148753\n",
      "  validation loss:\t\t0.266246\n",
      "Epoch 19 of 100 took 0.095s\n",
      "  training loss:\t\t0.146889\n",
      "  validation loss:\t\t0.298769\n",
      "Epoch 20 of 100 took 0.093s\n",
      "  training loss:\t\t0.152799\n",
      "  validation loss:\t\t0.293441\n",
      "Epoch 21 of 100 took 0.108s\n",
      "  training loss:\t\t0.146918\n",
      "  validation loss:\t\t0.275368\n",
      "Epoch 22 of 100 took 0.111s\n",
      "  training loss:\t\t0.144719\n",
      "  validation loss:\t\t0.294639\n",
      "Epoch 23 of 100 took 0.109s\n",
      "  training loss:\t\t0.142373\n",
      "  validation loss:\t\t0.247021\n",
      "Epoch 24 of 100 took 0.113s\n",
      "  training loss:\t\t0.142562\n",
      "  validation loss:\t\t0.267436\n",
      "Epoch 25 of 100 took 0.108s\n",
      "  training loss:\t\t0.142096\n",
      "  validation loss:\t\t0.263469\n",
      "Epoch 26 of 100 took 0.108s\n",
      "  training loss:\t\t0.138096\n",
      "  validation loss:\t\t0.270943\n",
      "Epoch 27 of 100 took 0.118s\n",
      "  training loss:\t\t0.136778\n",
      "  validation loss:\t\t0.321970\n",
      "Epoch 28 of 100 took 0.155s\n",
      "  training loss:\t\t0.137068\n",
      "  validation loss:\t\t0.326108\n",
      "Epoch 29 of 100 took 0.151s\n",
      "  training loss:\t\t0.133401\n",
      "  validation loss:\t\t0.260366\n",
      "Epoch 30 of 100 took 0.128s\n",
      "  training loss:\t\t0.137877\n",
      "  validation loss:\t\t0.259979\n",
      "Epoch 31 of 100 took 0.136s\n",
      "  training loss:\t\t0.130110\n",
      "  validation loss:\t\t0.276990\n",
      "Epoch 32 of 100 took 0.155s\n",
      "  training loss:\t\t0.131149\n",
      "  validation loss:\t\t0.284808\n",
      "Epoch 33 of 100 took 0.160s\n",
      "  training loss:\t\t0.129563\n",
      "  validation loss:\t\t0.330257\n",
      "Epoch 34 of 100 took 0.142s\n",
      "  training loss:\t\t0.128464\n",
      "  validation loss:\t\t0.475403\n",
      "Epoch 35 of 100 took 0.110s\n",
      "  training loss:\t\t0.126518\n",
      "  validation loss:\t\t0.263898\n",
      "Epoch 36 of 100 took 0.143s\n",
      "  training loss:\t\t0.126937\n",
      "  validation loss:\t\t0.292271\n",
      "Epoch 37 of 100 took 0.162s\n",
      "  training loss:\t\t0.123691\n",
      "  validation loss:\t\t0.267398\n",
      "Epoch 38 of 100 took 0.149s\n",
      "  training loss:\t\t0.125244\n",
      "  validation loss:\t\t0.284246\n",
      "Epoch 39 of 100 took 0.129s\n",
      "  training loss:\t\t0.130462\n",
      "  validation loss:\t\t0.281023\n",
      "Epoch 40 of 100 took 0.138s\n",
      "  training loss:\t\t0.125998\n",
      "  validation loss:\t\t0.269433\n",
      "Epoch 41 of 100 took 0.151s\n",
      "  training loss:\t\t0.124435\n",
      "  validation loss:\t\t0.299496\n",
      "Epoch 42 of 100 took 0.121s\n",
      "  training loss:\t\t0.118888\n",
      "  validation loss:\t\t0.286849\n",
      "Epoch 43 of 100 took 0.113s\n",
      "  training loss:\t\t0.122647\n",
      "  validation loss:\t\t0.517749\n",
      "Epoch 44 of 100 took 0.134s\n",
      "  training loss:\t\t0.120978\n",
      "  validation loss:\t\t0.286752\n",
      "Epoch 45 of 100 took 0.122s\n",
      "  training loss:\t\t0.119264\n",
      "  validation loss:\t\t0.295382\n",
      "Epoch 46 of 100 took 0.123s\n",
      "  training loss:\t\t0.125332\n",
      "  validation loss:\t\t0.272303\n",
      "Epoch 47 of 100 took 0.141s\n",
      "  training loss:\t\t0.125885\n",
      "  validation loss:\t\t0.276659\n",
      "Epoch 48 of 100 took 0.112s\n",
      "  training loss:\t\t0.120255\n",
      "  validation loss:\t\t0.287225\n",
      "Epoch 49 of 100 took 0.115s\n",
      "  training loss:\t\t0.115503\n",
      "  validation loss:\t\t0.266446\n",
      "Epoch 50 of 100 took 0.114s\n",
      "  training loss:\t\t0.115149\n",
      "  validation loss:\t\t0.291670\n",
      "Epoch 51 of 100 took 0.118s\n",
      "  training loss:\t\t0.115348\n",
      "  validation loss:\t\t0.323164\n",
      "Epoch 52 of 100 took 0.120s\n",
      "  training loss:\t\t0.116273\n",
      "  validation loss:\t\t0.315071\n",
      "Epoch 53 of 100 took 0.109s\n",
      "  training loss:\t\t0.113394\n",
      "  validation loss:\t\t0.312534\n",
      "Epoch 54 of 100 took 0.104s\n",
      "  training loss:\t\t0.110723\n",
      "  validation loss:\t\t0.476076\n",
      "Epoch 55 of 100 took 0.116s\n",
      "  training loss:\t\t0.112287\n",
      "  validation loss:\t\t0.286778\n",
      "Epoch 56 of 100 took 0.127s\n",
      "  training loss:\t\t0.117368\n",
      "  validation loss:\t\t0.287848\n",
      "Epoch 57 of 100 took 0.123s\n",
      "  training loss:\t\t0.113806\n",
      "  validation loss:\t\t0.317163\n",
      "Epoch 58 of 100 took 0.126s\n",
      "  training loss:\t\t0.111922\n",
      "  validation loss:\t\t0.270225\n",
      "Epoch 59 of 100 took 0.134s\n",
      "  training loss:\t\t0.108743\n",
      "  validation loss:\t\t0.276934\n",
      "Epoch 60 of 100 took 0.103s\n",
      "  training loss:\t\t0.106201\n",
      "  validation loss:\t\t0.280749\n",
      "Epoch 61 of 100 took 0.109s\n",
      "  training loss:\t\t0.105853\n",
      "  validation loss:\t\t0.285127\n",
      "Epoch 62 of 100 took 0.137s\n",
      "  training loss:\t\t0.107708\n",
      "  validation loss:\t\t0.293535\n",
      "Epoch 63 of 100 took 0.115s\n",
      "  training loss:\t\t0.107845\n",
      "  validation loss:\t\t0.302446\n",
      "Epoch 64 of 100 took 0.137s\n",
      "  training loss:\t\t0.105548\n",
      "  validation loss:\t\t0.310508\n",
      "Epoch 65 of 100 took 0.171s\n",
      "  training loss:\t\t0.102989\n",
      "  validation loss:\t\t0.396001\n",
      "Epoch 66 of 100 took 0.136s\n",
      "  training loss:\t\t0.105607\n",
      "  validation loss:\t\t0.342014\n",
      "Epoch 67 of 100 took 0.151s\n",
      "  training loss:\t\t0.099623\n",
      "  validation loss:\t\t0.312706\n",
      "Epoch 68 of 100 took 0.142s\n",
      "  training loss:\t\t0.100788\n",
      "  validation loss:\t\t0.559424\n",
      "Epoch 69 of 100 took 0.134s\n",
      "  training loss:\t\t0.101196\n",
      "  validation loss:\t\t0.269046\n",
      "Epoch 70 of 100 took 0.124s\n",
      "  training loss:\t\t0.097521\n",
      "  validation loss:\t\t0.294049\n",
      "Epoch 71 of 100 took 0.110s\n",
      "  training loss:\t\t0.101677\n",
      "  validation loss:\t\t0.370122\n",
      "Epoch 72 of 100 took 0.164s\n",
      "  training loss:\t\t0.095798\n",
      "  validation loss:\t\t0.288921\n",
      "Epoch 73 of 100 took 0.163s\n",
      "  training loss:\t\t0.103783\n",
      "  validation loss:\t\t0.342819\n",
      "Epoch 74 of 100 took 0.152s\n",
      "  training loss:\t\t0.093023\n",
      "  validation loss:\t\t0.376945\n",
      "Epoch 75 of 100 took 0.114s\n",
      "  training loss:\t\t0.098032\n",
      "  validation loss:\t\t0.335569\n",
      "Epoch 76 of 100 took 0.104s\n",
      "  training loss:\t\t0.120020\n",
      "  validation loss:\t\t0.281105\n",
      "Epoch 77 of 100 took 0.107s\n",
      "  training loss:\t\t0.106336\n",
      "  validation loss:\t\t0.280630\n",
      "Epoch 78 of 100 took 0.111s\n",
      "  training loss:\t\t0.107566\n",
      "  validation loss:\t\t0.281838\n",
      "Epoch 79 of 100 took 0.107s\n",
      "  training loss:\t\t0.100669\n",
      "  validation loss:\t\t0.300206\n",
      "Epoch 80 of 100 took 0.124s\n",
      "  training loss:\t\t0.098274\n",
      "  validation loss:\t\t0.282256\n",
      "Epoch 81 of 100 took 0.124s\n",
      "  training loss:\t\t0.095461\n",
      "  validation loss:\t\t0.283181\n",
      "Epoch 82 of 100 took 0.105s\n",
      "  training loss:\t\t0.092788\n",
      "  validation loss:\t\t0.312414\n",
      "Epoch 83 of 100 took 0.102s\n",
      "  training loss:\t\t0.095370\n",
      "  validation loss:\t\t0.290758\n",
      "Epoch 84 of 100 took 0.112s\n",
      "  training loss:\t\t0.094477\n",
      "  validation loss:\t\t0.314039\n",
      "Epoch 85 of 100 took 0.105s\n",
      "  training loss:\t\t0.091820\n",
      "  validation loss:\t\t0.320169\n",
      "Epoch 86 of 100 took 0.102s\n",
      "  training loss:\t\t0.089080\n",
      "  validation loss:\t\t0.296136\n",
      "Epoch 87 of 100 took 0.109s\n",
      "  training loss:\t\t0.088262\n",
      "  validation loss:\t\t0.356387\n",
      "Epoch 88 of 100 took 0.106s\n",
      "  training loss:\t\t0.086716\n",
      "  validation loss:\t\t0.390722\n",
      "Epoch 89 of 100 took 0.104s\n",
      "  training loss:\t\t0.098461\n",
      "  validation loss:\t\t0.317295\n",
      "Epoch 90 of 100 took 0.112s\n",
      "  training loss:\t\t0.092690\n",
      "  validation loss:\t\t0.293122\n",
      "Epoch 91 of 100 took 0.104s\n",
      "  training loss:\t\t0.087708\n",
      "  validation loss:\t\t0.379016\n",
      "Epoch 92 of 100 took 0.117s\n",
      "  training loss:\t\t0.095240\n",
      "  validation loss:\t\t0.310450\n",
      "Epoch 93 of 100 took 0.103s\n",
      "  training loss:\t\t0.094383\n",
      "  validation loss:\t\t0.280790\n",
      "Epoch 94 of 100 took 0.113s\n",
      "  training loss:\t\t0.087775\n",
      "  validation loss:\t\t0.340009\n",
      "Epoch 95 of 100 took 0.105s\n",
      "  training loss:\t\t0.091108\n",
      "  validation loss:\t\t0.274870\n",
      "Epoch 96 of 100 took 0.103s\n",
      "  training loss:\t\t0.088719\n",
      "  validation loss:\t\t0.435992\n",
      "Epoch 97 of 100 took 0.109s\n",
      "  training loss:\t\t0.086180\n",
      "  validation loss:\t\t0.362796\n",
      "Epoch 98 of 100 took 0.105s\n",
      "  training loss:\t\t0.086014\n",
      "  validation loss:\t\t0.293281\n",
      "Epoch 99 of 100 took 0.110s\n",
      "  training loss:\t\t0.084016\n",
      "  validation loss:\t\t0.292101\n",
      "Epoch 100 of 100 took 0.104s\n",
      "  training loss:\t\t0.086418\n",
      "  validation loss:\t\t0.294459\n",
      "Final results:\n",
      "  test loss:\t\t\t0.430685\n"
     ]
    }
   ],
   "source": [
    "# This is a trial of buiding neural network based on 2014 dataset. \n",
    "# It aims for debuging. The performance of network is not considered.\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "def load_dataset():\n",
    "    npdata = np.load('data2/demo_2014_numpy_standardised.npy')\n",
    "#     (m,n) = np.shape(npdata)\n",
    "#     npdata = npdata.reshape((m,1,n))\n",
    "    npdata = npdata.astype(np.float32)\n",
    "    X_train, X_val = npdata[0:4000, 0:53].reshape(4000,1,53), npdata[4000:4800, 0:53].reshape(800,1,53)\n",
    "    y_train, y_val = npdata[0:4000, 56].reshape(4000,1), npdata[4000:4800, 56].reshape(800,1)\n",
    "    x_test, y_test = npdata[4800:5625, 0:53].reshape(825,1,53), npdata[4800:5625, 56].reshape(825,1)\n",
    "    #print (X_train.shape, y_train.shape, X_val.shape, y_val.shape, npdata.shape)\n",
    "    return X_train, y_train, X_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "# ##################### Build the neural network model #######################\n",
    "# This script supports three types of models. For each one, we define a\n",
    "# function that takes a Theano variable representing the input and returns\n",
    "# the output layer of a neural network model built in Lasagne.\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "   \n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    # linking it to the given Theano variable `input_var`, if any:\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None,1,53),\n",
    "                                     input_var=input_var)\n",
    "\n",
    "    # Add a fully-connected layer of 100 units, using the linear rectifier, and\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # Another 100-unit layer:\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # Finally, we'll add the fully-connected output layer\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid2, num_units=1,\n",
    "            nonlinearity=lasagne.nonlinearities.linear)\n",
    "\n",
    "    # Each layer is linked to its incoming layer(s), so we only need to pass\n",
    "    # the output layer to give access to a network in Lasagne:\n",
    "    return l_out\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "# ############################## Main program ################################\n",
    "# Everything else will be handled in our main program now. We could pull out\n",
    "# more functions to better separate the code, but it wouldn't make it any\n",
    "# easier to read.\n",
    "\n",
    "def main(model='mlp', num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor3('inputs')\n",
    "    target_var = T.matrix('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    \n",
    "    network = build_mlp(input_var)\n",
    "    \n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.squared_error(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    # no accuracy for regression\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast = True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], test_loss, allow_input_downcast = True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 10, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, y_val.shape[0], shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            #print (inputs.shape, targets.shape)\n",
    "            err = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        \n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if ('--help' in sys.argv) or ('-h' in sys.argv):\n",
    "        print(\"Trains a neural network on MNIST using Lasagne.\")\n",
    "        print(\"Usage: %s [MODEL [EPOCHS]]\" % sys.argv[0])\n",
    "        print()\n",
    "        print(\"MODEL: 'mlp' for a simple Multi-Layer Perceptron (MLP),\")\n",
    "        print(\"       'custom_mlp:DEPTH,WIDTH,DROP_IN,DROP_HID' for an MLP\")\n",
    "        print(\"       with DEPTH hidden layers of WIDTH units, DROP_IN\")\n",
    "        print(\"       input dropout and DROP_HID hidden dropout,\")\n",
    "        print(\"       'cnn' for a simple Convolutional Neural Network (CNN).\")\n",
    "        print(\"EPOCHS: number of training epochs to perform (default: 500)\")\n",
    "    else:\n",
    "        kwargs = {}\n",
    "        if len(sys.argv) > 1:\n",
    "            kwargs['model'] = 'mlp'\n",
    "        if len(sys.argv) > 2:\n",
    "            kwargs['num_epochs'] = 100\n",
    "        main(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data standardisation - the rest of the years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data cleaning - remove missing data in each year\n",
    "2. Data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in 2009:  6638\n",
      "The number of samples in 2010:  6499\n",
      "The number of samples in 2011:  5819\n",
      "The number of samples in 2012:  5607\n",
      "The number of samples in 2013:  5388\n",
      "The number of samples in 2014:  5625\n",
      "The number of samples in 2015:  5537\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def one_hot_code(encode, column, start, npdata):\n",
    "    rawdata = df[column].values\n",
    "    i = 0\n",
    "    for j in rawdata:\n",
    "        npdata[i, start + encode[j]] = 1\n",
    "        i+=1\n",
    "\n",
    "years = ['2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "for year in years:\n",
    "    df = pd.read_csv('data2/Accidents_kis_' + year + '.csv')\n",
    "    ## select features\n",
    "    df = df[['Number_of_Vehicles', 'Date', 'Time','Longitude', \n",
    "             'Latitude', 'Speed_limit', 'Junction_Detail', 'Junction_Control', 'Light_Conditions', \n",
    "             'Weather_Conditions','Road_Surface_Conditions', 'Special_Conditions_at_Site', \n",
    "             'Urban_or_Rural_Area', 'Accident_Severity','Number_of_KIS']]\n",
    "\n",
    "    ## drop missing data\n",
    "    # weather_Conditions\n",
    "    df = df.drop(df[df.Weather_Conditions < 0].index)\n",
    "    # Road_Surface_Conditions\n",
    "    df = df.drop(df[df.Road_Surface_Conditions < 0].index)\n",
    "    # Special_Conditions_at_site\n",
    "    df = df.drop(df[df.Special_Conditions_at_Site < 0].index)\n",
    "    # Urban_or_Rural_Area\n",
    "    df = df.drop(df[df.Urban_or_Rural_Area < 0].index)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print ('The number of samples in '+year+ ': ', len(df))\n",
    "    ## Create numpy array\n",
    "    npdata = np.zeros(shape=(len(df),57))  # features(0-52)|Accident_severity(53-55)|number_of_kis\n",
    "\n",
    "    ## number of vehicles\n",
    "    npdata[:,0] = df['Number_of_Vehicles'].values\n",
    "\n",
    "    ## month, week, weekday\n",
    "    day = df['Date'].values\n",
    "    i = 0\n",
    "    for d in day:\n",
    "        d = time.strptime(d, '%d/%m/%Y')\n",
    "        npdata[i,1] = time.strftime('%m', d) # month\n",
    "        npdata[i,2] = time.strftime('%W', d) # week\n",
    "        npdata[i,3] = time.strftime('%w', d) # weekday\n",
    "        i += 1\n",
    "\n",
    "    ## time1 and time2 (sin and cos)\n",
    "    tt = df['Time'].values\n",
    "    i = 0\n",
    "    for t in tt:\n",
    "        hm = t.split(':')\n",
    "        t = float(hm[0])*60 + float(hm[1])\n",
    "        npdata[i,4] = math.sin(2* math.pi * t/1440)\n",
    "        npdata[i,5] = math.cos(2* math.pi * t/1440)\n",
    "\n",
    "    ## longitude and latitude\n",
    "    npdata[:,6] = df['Longitude'].values\n",
    "    npdata[:,7] = df['Latitude'].values\n",
    "\n",
    "    ## speed limit\n",
    "    npdata[:,8] = df['Speed_limit'].values\n",
    "\n",
    "    ## Junction Detail\n",
    "    encode = {0: 0, 1:1, 2: 2, 3:3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "    one_hot_code(encode = encode, column = 'Junction_Detail', start = 9, npdata = npdata)\n",
    "\n",
    "    ## Junction Control\n",
    "    junc = df['Junction_Control'].values\n",
    "    i = 0\n",
    "    for j in junc:\n",
    "        if j == -1:\n",
    "            npdata[i,18] = 1\n",
    "        else:\n",
    "            npdata[i, 18+j] = 1\n",
    "        i+=1\n",
    "\n",
    "    ## Light Conditions\n",
    "    encode = {1:0, 4:1, 5:2, 6:3, 7:4}\n",
    "    one_hot_code(encode = encode, column = 'Light_Conditions', start = 23, npdata = npdata)\n",
    "\n",
    "    ## Weather Conditions\n",
    "    encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "    one_hot_code(encode = encode, column = 'Weather_Conditions', start = 28, npdata = npdata)\n",
    "\n",
    "    ## Road surface conditions\n",
    "    encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6}\n",
    "    one_hot_code(encode = encode, column = 'Road_Surface_Conditions', start = 37, npdata = npdata)\n",
    "\n",
    "    ## Special conditions at site\n",
    "    encode = {0:0, 1: 1, 2: 2, 3: 3, 4:4, 5:5, 6:6, 7:7}\n",
    "    one_hot_code(encode = encode, column = 'Special_Conditions_at_Site', start = 44, npdata = npdata)\n",
    "\n",
    "    ## urban or rural\n",
    "    ur = df['Urban_or_Rural_Area'].values\n",
    "    i = 0\n",
    "    for j in ur:\n",
    "        if j==1:\n",
    "            npdata[i, 52] = 1\n",
    "        else:\n",
    "            npdata[i, 52] = -1\n",
    "        i+=1\n",
    "\n",
    "    ## Accident severity\n",
    "    encode = {1:0, 2:1, 3:2}\n",
    "    one_hot_code(encode = encode, column = 'Accident_Severity', start = 53, npdata = npdata)\n",
    "\n",
    "    ## Number of KIS\n",
    "    npdata[:,56] = df['Number_of_KIS'].values\n",
    "\n",
    "    np.save('data3/Accidents_'+ year + '_numpy_encoded.npy', npdata) # save each year's data as numpy array\n",
    "    #implement encoding, wait for normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009: (6638, 57)\n",
      "2010: (6499, 57)\n",
      "2011: (5819, 57)\n",
      "2012: (5607, 57)\n",
      "2013: (5388, 57)\n",
      "2014: (5625, 57)\n",
      "2015: (5537, 57)\n",
      "all data: (41113, 57)\n"
     ]
    }
   ],
   "source": [
    "npdata = np.load('data3/Accidents_2009_numpy_encoded.npy') # initialise numpy array\n",
    "print ('2009:', npdata.shape)\n",
    "years = ['2010', '2011', '2012', '2013', '2014', '2015']\n",
    "for year in years:\n",
    "    newdata = np.load('data3/Accidents_' + year + '_numpy_encoded.npy')\n",
    "    print (year+':', newdata.shape)\n",
    "    npdata = np.vstack([npdata,newdata])\n",
    "    \n",
    "print ('all data:', npdata.shape)\n",
    "\n",
    "clm = [0, 1, 2, 3,6,7,8]\n",
    "for c in clm:\n",
    "    norm = npdata[:,c]\n",
    "    norm = (norm - np.mean(norm))/np.std(norm)\n",
    "    npdata[:,c] = norm\n",
    "\n",
    "np.save('data3/Accidents_allyear_numpy_standardised.npy', npdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - 1\n",
    "- predict the number of KIS\n",
    "- training data: 2009-2013\n",
    "- validaton data: 2014\n",
    "- test data: 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 100 took 1.418s\n",
      "  training loss:\t\t0.249183\n",
      "  validation loss:\t\t0.204126\n",
      "Epoch 2 of 100 took 1.336s\n",
      "  training loss:\t\t0.220199\n",
      "  validation loss:\t\t0.201780\n",
      "Epoch 3 of 100 took 1.346s\n",
      "  training loss:\t\t0.218188\n",
      "  validation loss:\t\t0.200462\n",
      "Epoch 4 of 100 took 1.311s\n",
      "  training loss:\t\t0.216902\n",
      "  validation loss:\t\t0.199565\n",
      "Epoch 5 of 100 took 1.398s\n",
      "  training loss:\t\t0.215893\n",
      "  validation loss:\t\t0.198879\n",
      "Epoch 6 of 100 took 1.534s\n",
      "  training loss:\t\t0.215049\n",
      "  validation loss:\t\t0.198326\n",
      "Epoch 7 of 100 took 1.491s\n",
      "  training loss:\t\t0.214338\n",
      "  validation loss:\t\t0.197803\n",
      "Epoch 8 of 100 took 1.414s\n",
      "  training loss:\t\t0.213764\n",
      "  validation loss:\t\t0.197385\n",
      "Epoch 9 of 100 took 1.420s\n",
      "  training loss:\t\t0.213193\n",
      "  validation loss:\t\t0.197023\n",
      "Epoch 10 of 100 took 1.464s\n",
      "  training loss:\t\t0.212713\n",
      "  validation loss:\t\t0.196660\n",
      "Epoch 11 of 100 took 1.534s\n",
      "  training loss:\t\t0.212226\n",
      "  validation loss:\t\t0.196378\n",
      "Epoch 12 of 100 took 1.433s\n",
      "  training loss:\t\t0.211821\n",
      "  validation loss:\t\t0.196126\n",
      "Epoch 13 of 100 took 1.489s\n",
      "  training loss:\t\t0.211478\n",
      "  validation loss:\t\t0.195869\n",
      "Epoch 14 of 100 took 1.298s\n",
      "  training loss:\t\t0.211144\n",
      "  validation loss:\t\t0.195619\n",
      "Epoch 15 of 100 took 1.347s\n",
      "  training loss:\t\t0.210794\n",
      "  validation loss:\t\t0.195417\n",
      "Epoch 16 of 100 took 1.368s\n",
      "  training loss:\t\t0.210524\n",
      "  validation loss:\t\t0.195225\n",
      "Epoch 17 of 100 took 1.320s\n",
      "  training loss:\t\t0.210265\n",
      "  validation loss:\t\t0.195068\n",
      "Epoch 18 of 100 took 1.347s\n",
      "  training loss:\t\t0.210018\n",
      "  validation loss:\t\t0.194892\n",
      "Epoch 19 of 100 took 1.313s\n",
      "  training loss:\t\t0.209751\n",
      "  validation loss:\t\t0.194831\n",
      "Epoch 20 of 100 took 1.355s\n",
      "  training loss:\t\t0.209567\n",
      "  validation loss:\t\t0.194620\n",
      "Epoch 21 of 100 took 1.340s\n",
      "  training loss:\t\t0.209379\n",
      "  validation loss:\t\t0.194490\n",
      "Epoch 22 of 100 took 1.316s\n",
      "  training loss:\t\t0.209176\n",
      "  validation loss:\t\t0.194377\n",
      "Epoch 23 of 100 took 1.316s\n",
      "  training loss:\t\t0.208967\n",
      "  validation loss:\t\t0.194334\n",
      "Epoch 24 of 100 took 1.367s\n",
      "  training loss:\t\t0.208844\n",
      "  validation loss:\t\t0.194177\n",
      "Epoch 25 of 100 took 1.391s\n",
      "  training loss:\t\t0.208544\n",
      "  validation loss:\t\t0.194082\n",
      "Epoch 26 of 100 took 1.608s\n",
      "  training loss:\t\t0.208555\n",
      "  validation loss:\t\t0.193971\n",
      "Epoch 27 of 100 took 1.673s\n",
      "  training loss:\t\t0.208389\n",
      "  validation loss:\t\t0.193885\n",
      "Epoch 28 of 100 took 1.461s\n",
      "  training loss:\t\t0.208240\n",
      "  validation loss:\t\t0.193809\n",
      "Epoch 29 of 100 took 1.482s\n",
      "  training loss:\t\t0.208145\n",
      "  validation loss:\t\t0.193727\n",
      "Epoch 30 of 100 took 1.473s\n",
      "  training loss:\t\t0.208001\n",
      "  validation loss:\t\t0.193702\n",
      "Epoch 31 of 100 took 1.506s\n",
      "  training loss:\t\t0.207867\n",
      "  validation loss:\t\t0.193587\n",
      "Epoch 32 of 100 took 1.439s\n",
      "  training loss:\t\t0.207776\n",
      "  validation loss:\t\t0.193532\n",
      "Epoch 33 of 100 took 1.504s\n",
      "  training loss:\t\t0.207656\n",
      "  validation loss:\t\t0.193493\n",
      "Epoch 34 of 100 took 1.486s\n",
      "  training loss:\t\t0.207568\n",
      "  validation loss:\t\t0.193432\n",
      "Epoch 35 of 100 took 1.490s\n",
      "  training loss:\t\t0.207413\n",
      "  validation loss:\t\t0.193369\n",
      "Epoch 36 of 100 took 1.475s\n",
      "  training loss:\t\t0.207387\n",
      "  validation loss:\t\t0.193315\n",
      "Epoch 37 of 100 took 1.477s\n",
      "  training loss:\t\t0.207289\n",
      "  validation loss:\t\t0.193347\n",
      "Epoch 38 of 100 took 1.493s\n",
      "  training loss:\t\t0.207086\n",
      "  validation loss:\t\t0.193197\n",
      "Epoch 39 of 100 took 1.473s\n",
      "  training loss:\t\t0.207074\n",
      "  validation loss:\t\t0.193211\n",
      "Epoch 40 of 100 took 1.477s\n",
      "  training loss:\t\t0.207025\n",
      "  validation loss:\t\t0.193130\n",
      "Epoch 41 of 100 took 1.465s\n",
      "  training loss:\t\t0.206952\n",
      "  validation loss:\t\t0.193074\n",
      "Epoch 42 of 100 took 1.399s\n",
      "  training loss:\t\t0.206850\n",
      "  validation loss:\t\t0.193026\n",
      "Epoch 43 of 100 took 1.445s\n",
      "  training loss:\t\t0.206792\n",
      "  validation loss:\t\t0.193008\n",
      "Epoch 44 of 100 took 1.567s\n",
      "  training loss:\t\t0.206688\n",
      "  validation loss:\t\t0.192949\n",
      "Epoch 45 of 100 took 2.002s\n",
      "  training loss:\t\t0.206642\n",
      "  validation loss:\t\t0.192929\n",
      "Epoch 46 of 100 took 1.565s\n",
      "  training loss:\t\t0.206552\n",
      "  validation loss:\t\t0.192895\n",
      "Epoch 47 of 100 took 1.771s\n",
      "  training loss:\t\t0.206486\n",
      "  validation loss:\t\t0.192883\n",
      "Epoch 48 of 100 took 1.577s\n",
      "  training loss:\t\t0.206413\n",
      "  validation loss:\t\t0.192823\n",
      "Epoch 49 of 100 took 1.512s\n",
      "  training loss:\t\t0.206348\n",
      "  validation loss:\t\t0.192805\n",
      "Epoch 50 of 100 took 1.605s\n",
      "  training loss:\t\t0.206289\n",
      "  validation loss:\t\t0.192771\n",
      "Epoch 51 of 100 took 1.518s\n",
      "  training loss:\t\t0.206228\n",
      "  validation loss:\t\t0.192740\n",
      "Epoch 52 of 100 took 1.553s\n",
      "  training loss:\t\t0.206157\n",
      "  validation loss:\t\t0.192746\n",
      "Epoch 53 of 100 took 1.555s\n",
      "  training loss:\t\t0.206104\n",
      "  validation loss:\t\t0.192674\n",
      "Epoch 54 of 100 took 1.546s\n",
      "  training loss:\t\t0.206050\n",
      "  validation loss:\t\t0.192671\n",
      "Epoch 55 of 100 took 1.560s\n",
      "  training loss:\t\t0.205986\n",
      "  validation loss:\t\t0.192639\n",
      "Epoch 56 of 100 took 1.504s\n",
      "  training loss:\t\t0.205917\n",
      "  validation loss:\t\t0.192665\n",
      "Epoch 57 of 100 took 1.433s\n",
      "  training loss:\t\t0.205845\n",
      "  validation loss:\t\t0.192572\n",
      "Epoch 58 of 100 took 1.397s\n",
      "  training loss:\t\t0.205797\n",
      "  validation loss:\t\t0.192555\n",
      "Epoch 59 of 100 took 1.452s\n",
      "  training loss:\t\t0.205742\n",
      "  validation loss:\t\t0.192599\n",
      "Epoch 60 of 100 took 1.449s\n",
      "  training loss:\t\t0.205709\n",
      "  validation loss:\t\t0.192543\n",
      "Epoch 61 of 100 took 1.607s\n",
      "  training loss:\t\t0.205679\n",
      "  validation loss:\t\t0.192499\n",
      "Epoch 62 of 100 took 1.472s\n",
      "  training loss:\t\t0.205621\n",
      "  validation loss:\t\t0.192468\n",
      "Epoch 63 of 100 took 1.507s\n",
      "  training loss:\t\t0.205575\n",
      "  validation loss:\t\t0.192443\n",
      "Epoch 64 of 100 took 1.512s\n",
      "  training loss:\t\t0.205473\n",
      "  validation loss:\t\t0.192449\n",
      "Epoch 65 of 100 took 1.501s\n",
      "  training loss:\t\t0.205462\n",
      "  validation loss:\t\t0.192402\n",
      "Epoch 66 of 100 took 1.427s\n",
      "  training loss:\t\t0.205390\n",
      "  validation loss:\t\t0.192463\n",
      "Epoch 67 of 100 took 1.442s\n",
      "  training loss:\t\t0.205380\n",
      "  validation loss:\t\t0.192373\n",
      "Epoch 68 of 100 took 1.401s\n",
      "  training loss:\t\t0.205325\n",
      "  validation loss:\t\t0.192347\n",
      "Epoch 69 of 100 took 1.361s\n",
      "  training loss:\t\t0.205287\n",
      "  validation loss:\t\t0.192332\n",
      "Epoch 70 of 100 took 1.444s\n",
      "  training loss:\t\t0.205246\n",
      "  validation loss:\t\t0.192330\n",
      "Epoch 71 of 100 took 1.468s\n",
      "  training loss:\t\t0.205144\n",
      "  validation loss:\t\t0.192319\n",
      "Epoch 72 of 100 took 1.746s\n",
      "  training loss:\t\t0.205144\n",
      "  validation loss:\t\t0.192292\n",
      "Epoch 73 of 100 took 1.797s\n",
      "  training loss:\t\t0.205075\n",
      "  validation loss:\t\t0.192245\n",
      "Epoch 74 of 100 took 1.641s\n",
      "  training loss:\t\t0.205050\n",
      "  validation loss:\t\t0.192232\n",
      "Epoch 75 of 100 took 1.510s\n",
      "  training loss:\t\t0.205003\n",
      "  validation loss:\t\t0.192226\n",
      "Epoch 76 of 100 took 1.453s\n",
      "  training loss:\t\t0.204709\n",
      "  validation loss:\t\t0.192243\n",
      "Epoch 77 of 100 took 1.375s\n",
      "  training loss:\t\t0.204837\n",
      "  validation loss:\t\t0.192199\n",
      "Epoch 78 of 100 took 1.472s\n",
      "  training loss:\t\t0.204900\n",
      "  validation loss:\t\t0.192160\n",
      "Epoch 79 of 100 took 1.540s\n",
      "  training loss:\t\t0.204862\n",
      "  validation loss:\t\t0.192146\n",
      "Epoch 80 of 100 took 1.766s\n",
      "  training loss:\t\t0.204788\n",
      "  validation loss:\t\t0.192155\n",
      "Epoch 81 of 100 took 1.468s\n",
      "  training loss:\t\t0.204739\n",
      "  validation loss:\t\t0.192238\n",
      "Epoch 82 of 100 took 1.630s\n",
      "  training loss:\t\t0.204752\n",
      "  validation loss:\t\t0.192140\n",
      "Epoch 83 of 100 took 1.497s\n",
      "  training loss:\t\t0.204716\n",
      "  validation loss:\t\t0.192083\n",
      "Epoch 84 of 100 took 1.582s\n",
      "  training loss:\t\t0.204665\n",
      "  validation loss:\t\t0.192084\n",
      "Epoch 85 of 100 took 1.451s\n",
      "  training loss:\t\t0.204637\n",
      "  validation loss:\t\t0.192058\n",
      "Epoch 86 of 100 took 1.460s\n",
      "  training loss:\t\t0.204563\n",
      "  validation loss:\t\t0.192059\n",
      "Epoch 87 of 100 took 1.547s\n",
      "  training loss:\t\t0.204552\n",
      "  validation loss:\t\t0.192042\n",
      "Epoch 88 of 100 took 1.569s\n",
      "  training loss:\t\t0.204499\n",
      "  validation loss:\t\t0.192039\n",
      "Epoch 89 of 100 took 1.776s\n",
      "  training loss:\t\t0.204493\n",
      "  validation loss:\t\t0.192020\n",
      "Epoch 90 of 100 took 1.968s\n",
      "  training loss:\t\t0.204455\n",
      "  validation loss:\t\t0.192003\n",
      "Epoch 91 of 100 took 1.570s\n",
      "  training loss:\t\t0.204395\n",
      "  validation loss:\t\t0.192003\n",
      "Epoch 92 of 100 took 1.570s\n",
      "  training loss:\t\t0.204338\n",
      "  validation loss:\t\t0.192024\n",
      "Epoch 93 of 100 took 1.591s\n",
      "  training loss:\t\t0.204371\n",
      "  validation loss:\t\t0.191982\n",
      "Epoch 94 of 100 took 1.481s\n",
      "  training loss:\t\t0.204278\n",
      "  validation loss:\t\t0.192010\n",
      "Epoch 95 of 100 took 1.645s\n",
      "  training loss:\t\t0.204278\n",
      "  validation loss:\t\t0.191958\n",
      "Epoch 96 of 100 took 1.972s\n",
      "  training loss:\t\t0.204212\n",
      "  validation loss:\t\t0.191978\n",
      "Epoch 97 of 100 took 1.593s\n",
      "  training loss:\t\t0.204207\n",
      "  validation loss:\t\t0.191956\n",
      "Epoch 98 of 100 took 1.615s\n",
      "  training loss:\t\t0.204138\n",
      "  validation loss:\t\t0.191904\n",
      "Epoch 99 of 100 took 1.731s\n",
      "  training loss:\t\t0.204111\n",
      "  validation loss:\t\t0.191918\n",
      "Epoch 100 of 100 took 1.883s\n",
      "  training loss:\t\t0.204114\n",
      "  validation loss:\t\t0.191882\n",
      "Final results:\n",
      "  test loss:\t\t\t0.186792\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "def load_dataset():\n",
    "    npdata = np.load('data3/Accidents_allyear_numpy_standardised.npy')\n",
    "    npdata = npdata.astype(np.float32)\n",
    "    X_train, X_val = npdata[0:29951, 0:53].reshape(29951,1,53), npdata[29951:35576, 0:53].reshape(5625,1,53)\n",
    "    y_train, y_val = npdata[0:29951, 56].reshape(29951,1), npdata[29951:35576, 56].reshape(5625,1)\n",
    "    x_test, y_test = npdata[35576:41113, 0:53].reshape(5537,1,53), npdata[35576:41113, 56].reshape(5537,1 )\n",
    "    #print (X_train.shape, y_train.shape, X_val.shape, y_val.shape, npdata.shape)\n",
    "    return X_train, y_train, X_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "    \n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None,1,53),\n",
    "                                     input_var=input_var)\n",
    "\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "   \n",
    "    # Another 200-unit layer:\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1, num_units=200,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid2, num_units=1,\n",
    "            nonlinearity=lasagne.nonlinearities.linear)\n",
    "\n",
    "    # Each layer is linked to its incoming layer(s), so we only need to pass\n",
    "    # the output layer to give access to a network in Lasagne:\n",
    "    return l_out\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "# ############################## Main program ################################\n",
    "# Everything else will be handled in our main program now. We could pull out\n",
    "# more functions to better separate the code, but it wouldn't make it any\n",
    "# easier to read.\n",
    "\n",
    "def main(model='mlp', num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor3('inputs')\n",
    "    target_var = T.matrix('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    if model == 'mlp':\n",
    "        network = build_mlp(input_var)\n",
    "    elif model.startswith('custom_mlp:'):\n",
    "        depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
    "        network = build_custom_mlp(input_var, int(depth), int(width),\n",
    "                                   float(drop_in), float(drop_hid))\n",
    "    elif model == 'cnn':\n",
    "        network = build_cnn(input_var)\n",
    "    else:\n",
    "        print(\"Unrecognized model type %r.\" % model)\n",
    "        return\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.00001, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.squared_error(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast = True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], test_loss, allow_input_downcast = True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 10, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, y_val.shape[0], shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            #print (inputs.shape, targets.shape)\n",
    "            err = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        \n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if ('--help' in sys.argv) or ('-h' in sys.argv):\n",
    "        print(\"Trains a neural network on MNIST using Lasagne.\")\n",
    "        print(\"Usage: %s [MODEL [EPOCHS]]\" % sys.argv[0])\n",
    "        print()\n",
    "        print(\"MODEL: 'mlp' for a simple Multi-Layer Perceptron (MLP),\")\n",
    "        print(\"       'custom_mlp:DEPTH,WIDTH,DROP_IN,DROP_HID' for an MLP\")\n",
    "        print(\"       with DEPTH hidden layers of WIDTH units, DROP_IN\")\n",
    "        print(\"       input dropout and DROP_HID hidden dropout,\")\n",
    "        print(\"       'cnn' for a simple Convolutional Neural Network (CNN).\")\n",
    "        print(\"EPOCHS: number of training epochs to perform (default: 500)\")\n",
    "    else:\n",
    "        kwargs = {}\n",
    "        if len(sys.argv) > 1:\n",
    "            kwargs['model'] = 'mlp'\n",
    "        if len(sys.argv) > 2:\n",
    "            kwargs['num_epochs'] = 100\n",
    "        main(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not encode label of accident severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in 2009:  6638\n",
      "The number of samples in 2010:  6499\n",
      "The number of samples in 2011:  5819\n",
      "The number of samples in 2012:  5607\n",
      "The number of samples in 2013:  5388\n",
      "The number of samples in 2014:  5625\n",
      "The number of samples in 2015:  5537\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def one_hot_code(encode, column, start, npdata):\n",
    "    rawdata = df[column].values\n",
    "    i = 0\n",
    "    for j in rawdata:\n",
    "        npdata[i, start + encode[j]] = 1\n",
    "        i+=1\n",
    "\n",
    "years = ['2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "for year in years:\n",
    "    df = pd.read_csv('data2/Accidents_kis_' + year + '.csv')\n",
    "    ## select features\n",
    "    df = df[['Number_of_Vehicles', 'Date', 'Time','Longitude', \n",
    "             'Latitude', 'Speed_limit', 'Junction_Detail', 'Junction_Control', 'Light_Conditions', \n",
    "             'Weather_Conditions','Road_Surface_Conditions', 'Special_Conditions_at_Site', \n",
    "             'Urban_or_Rural_Area', 'Accident_Severity','Number_of_KIS']]\n",
    "\n",
    "    ## drop missing data\n",
    "    # weather_Conditions\n",
    "    df = df.drop(df[df.Weather_Conditions < 0].index)\n",
    "    # Road_Surface_Conditions\n",
    "    df = df.drop(df[df.Road_Surface_Conditions < 0].index)\n",
    "    # Special_Conditions_at_site\n",
    "    df = df.drop(df[df.Special_Conditions_at_Site < 0].index)\n",
    "    # Urban_or_Rural_Area\n",
    "    df = df.drop(df[df.Urban_or_Rural_Area < 0].index)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print ('The number of samples in '+year+ ': ', len(df))\n",
    "    ## Create numpy array\n",
    "    npdata = np.zeros(shape=(len(df),55))  # features(0-52)|Accident_severity(53-55)|number_of_kis\n",
    "\n",
    "    ## number of vehicles\n",
    "    npdata[:,0] = df['Number_of_Vehicles'].values\n",
    "\n",
    "    ## month, week, weekday\n",
    "    day = df['Date'].values\n",
    "    i = 0\n",
    "    for d in day:\n",
    "        d = time.strptime(d, '%d/%m/%Y')\n",
    "        npdata[i,1] = time.strftime('%m', d) # month\n",
    "        npdata[i,2] = time.strftime('%W', d) # week\n",
    "        npdata[i,3] = time.strftime('%w', d) # weekday\n",
    "        i += 1\n",
    "\n",
    "    ## time1 and time2 (sin and cos)\n",
    "    tt = df['Time'].values\n",
    "    i = 0\n",
    "    for t in tt:\n",
    "        hm = t.split(':')\n",
    "        t = float(hm[0])*60 + float(hm[1])\n",
    "        npdata[i,4] = math.sin(2* math.pi * t/1440)\n",
    "        npdata[i,5] = math.cos(2* math.pi * t/1440)\n",
    "\n",
    "    ## longitude and latitude\n",
    "    npdata[:,6] = df['Longitude'].values\n",
    "    npdata[:,7] = df['Latitude'].values\n",
    "\n",
    "    ## speed limit\n",
    "    npdata[:,8] = df['Speed_limit'].values\n",
    "\n",
    "    ## Junction Detail\n",
    "    encode = {0: 0, 1:1, 2: 2, 3:3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "    one_hot_code(encode = encode, column = 'Junction_Detail', start = 9, npdata = npdata)\n",
    "\n",
    "    ## Junction Control\n",
    "    junc = df['Junction_Control'].values\n",
    "    i = 0\n",
    "    for j in junc:\n",
    "        if j == -1:\n",
    "            npdata[i,18] = 1\n",
    "        else:\n",
    "            npdata[i, 18+j] = 1\n",
    "        i+=1\n",
    "\n",
    "    ## Light Conditions\n",
    "    encode = {1:0, 4:1, 5:2, 6:3, 7:4}\n",
    "    one_hot_code(encode = encode, column = 'Light_Conditions', start = 23, npdata = npdata)\n",
    "\n",
    "    ## Weather Conditions\n",
    "    encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "    one_hot_code(encode = encode, column = 'Weather_Conditions', start = 28, npdata = npdata)\n",
    "\n",
    "    ## Road surface conditions\n",
    "    encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6}\n",
    "    one_hot_code(encode = encode, column = 'Road_Surface_Conditions', start = 37, npdata = npdata)\n",
    "\n",
    "    ## Special conditions at site\n",
    "    encode = {0:0, 1: 1, 2: 2, 3: 3, 4:4, 5:5, 6:6, 7:7}\n",
    "    one_hot_code(encode = encode, column = 'Special_Conditions_at_Site', start = 44, npdata = npdata)\n",
    "\n",
    "    ## urban or rural\n",
    "    ur = df['Urban_or_Rural_Area'].values\n",
    "    i = 0\n",
    "    for j in ur:\n",
    "        if j==1:\n",
    "            npdata[i, 52] = 1\n",
    "        else:\n",
    "            npdata[i, 52] = -1\n",
    "        i+=1\n",
    "\n",
    "    ## Accident severity, do not encode\n",
    "    npdata[:,53] = df['Accident_Severity'].values\n",
    "\n",
    "    ## Number of KIS\n",
    "    npdata[:,54] = df['Number_of_KIS'].values\n",
    "\n",
    "    np.save('data3/Accidents_'+ year + '_numpy_encoded2.npy', npdata) # save each year's data as numpy array\n",
    "    #implement encoding, wait for normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009: (6638, 55)\n",
      "2010: (6499, 55)\n",
      "2011: (5819, 55)\n",
      "2012: (5607, 55)\n",
      "2013: (5388, 55)\n",
      "2014: (5625, 55)\n",
      "2015: (5537, 55)\n",
      "all data: (41113, 55)\n"
     ]
    }
   ],
   "source": [
    "npdata = np.load('data3/Accidents_2009_numpy_encoded2.npy') # initialise numpy array\n",
    "print ('2009:', npdata.shape)\n",
    "years = ['2010', '2011', '2012', '2013', '2014', '2015']\n",
    "for year in years:\n",
    "    newdata = np.load('data3/Accidents_' + year + '_numpy_encoded2.npy')\n",
    "    print (year+':', newdata.shape)\n",
    "    npdata = np.vstack([npdata,newdata])\n",
    "    \n",
    "print ('all data:', npdata.shape)\n",
    "\n",
    "clm = [0, 1, 2, 3,6,7,8]\n",
    "for c in clm:\n",
    "    norm = npdata[:,c]\n",
    "    norm = (norm - np.mean(norm))/np.std(norm)\n",
    "    npdata[:,c] = norm\n",
    "\n",
    "np.save('data3/Accidents_allyear_numpy_standardised2.npy', npdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npdata = np.load('data3/Accidents_allyear_numpy_standardised2.npy')\n",
    "sum(np.isnan(npdata)*1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accident severity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(29951, 1, 53) (29951,) (5625, 1, 53) (5625,) (41113, 55)\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 50 took 5.095s\n",
      "  training loss:\t\t0.415503\n",
      "  validation loss:\t\t0.405417\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 2 of 50 took 5.269s\n",
      "  training loss:\t\t0.403370\n",
      "  validation loss:\t\t0.400768\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 3 of 50 took 5.042s\n",
      "  training loss:\t\t0.400325\n",
      "  validation loss:\t\t0.400616\n",
      "  validation accuracy:\t\t87.94 %\n",
      "Epoch 4 of 50 took 5.154s\n",
      "  training loss:\t\t0.397930\n",
      "  validation loss:\t\t0.397971\n",
      "  validation accuracy:\t\t87.94 %\n",
      "Epoch 5 of 50 took 5.200s\n",
      "  training loss:\t\t0.396233\n",
      "  validation loss:\t\t0.399154\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 6 of 50 took 5.578s\n",
      "  training loss:\t\t0.395464\n",
      "  validation loss:\t\t0.397706\n",
      "  validation accuracy:\t\t87.94 %\n",
      "Epoch 7 of 50 took 5.659s\n",
      "  training loss:\t\t0.394347\n",
      "  validation loss:\t\t0.397335\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 8 of 50 took 5.593s\n",
      "  training loss:\t\t0.393513\n",
      "  validation loss:\t\t0.397011\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 9 of 50 took 5.301s\n",
      "  training loss:\t\t0.393015\n",
      "  validation loss:\t\t0.396661\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 10 of 50 took 5.223s\n",
      "  training loss:\t\t0.392276\n",
      "  validation loss:\t\t0.397134\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 11 of 50 took 5.235s\n",
      "  training loss:\t\t0.391627\n",
      "  validation loss:\t\t0.396257\n",
      "  validation accuracy:\t\t87.94 %\n",
      "Epoch 12 of 50 took 5.180s\n",
      "  training loss:\t\t0.391103\n",
      "  validation loss:\t\t0.398190\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 13 of 50 took 5.274s\n",
      "  training loss:\t\t0.390571\n",
      "  validation loss:\t\t0.396757\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 14 of 50 took 5.185s\n",
      "  training loss:\t\t0.390265\n",
      "  validation loss:\t\t0.397549\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 15 of 50 took 5.384s\n",
      "  training loss:\t\t0.389515\n",
      "  validation loss:\t\t0.396551\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 16 of 50 took 5.225s\n",
      "  training loss:\t\t0.388970\n",
      "  validation loss:\t\t0.396547\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 17 of 50 took 5.158s\n",
      "  training loss:\t\t0.389187\n",
      "  validation loss:\t\t0.397028\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 18 of 50 took 5.319s\n",
      "  training loss:\t\t0.388234\n",
      "  validation loss:\t\t0.397065\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 19 of 50 took 5.441s\n",
      "  training loss:\t\t0.387311\n",
      "  validation loss:\t\t0.400033\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 20 of 50 took 5.398s\n",
      "  training loss:\t\t0.386855\n",
      "  validation loss:\t\t0.398267\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 21 of 50 took 5.930s\n",
      "  training loss:\t\t0.386647\n",
      "  validation loss:\t\t0.398344\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 22 of 50 took 5.458s\n",
      "  training loss:\t\t0.386029\n",
      "  validation loss:\t\t0.398799\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 23 of 50 took 5.245s\n",
      "  training loss:\t\t0.385747\n",
      "  validation loss:\t\t0.397139\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 24 of 50 took 5.234s\n",
      "  training loss:\t\t0.384957\n",
      "  validation loss:\t\t0.398686\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 25 of 50 took 5.372s\n",
      "  training loss:\t\t0.384302\n",
      "  validation loss:\t\t0.396604\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 26 of 50 took 5.649s\n",
      "  training loss:\t\t0.384009\n",
      "  validation loss:\t\t0.401263\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 27 of 50 took 5.379s\n",
      "  training loss:\t\t0.383095\n",
      "  validation loss:\t\t0.401842\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 28 of 50 took 5.707s\n",
      "  training loss:\t\t0.382873\n",
      "  validation loss:\t\t0.397975\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 29 of 50 took 5.385s\n",
      "  training loss:\t\t0.382466\n",
      "  validation loss:\t\t0.401469\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 30 of 50 took 5.275s\n",
      "  training loss:\t\t0.381697\n",
      "  validation loss:\t\t0.397407\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 31 of 50 took 5.299s\n",
      "  training loss:\t\t0.381545\n",
      "  validation loss:\t\t0.402059\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 32 of 50 took 5.320s\n",
      "  training loss:\t\t0.381144\n",
      "  validation loss:\t\t0.398377\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 33 of 50 took 5.299s\n",
      "  training loss:\t\t0.380377\n",
      "  validation loss:\t\t0.397337\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 34 of 50 took 5.313s\n",
      "  training loss:\t\t0.380004\n",
      "  validation loss:\t\t0.398480\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 35 of 50 took 5.305s\n",
      "  training loss:\t\t0.379342\n",
      "  validation loss:\t\t0.401436\n",
      "  validation accuracy:\t\t87.86 %\n",
      "Epoch 36 of 50 took 5.412s\n",
      "  training loss:\t\t0.378443\n",
      "  validation loss:\t\t0.400677\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 37 of 50 took 5.300s\n",
      "  training loss:\t\t0.378269\n",
      "  validation loss:\t\t0.404368\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 38 of 50 took 5.295s\n",
      "  training loss:\t\t0.377258\n",
      "  validation loss:\t\t0.400487\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 39 of 50 took 5.217s\n",
      "  training loss:\t\t0.377038\n",
      "  validation loss:\t\t0.398994\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 40 of 50 took 5.223s\n",
      "  training loss:\t\t0.376171\n",
      "  validation loss:\t\t0.405336\n",
      "  validation accuracy:\t\t87.94 %\n",
      "Epoch 41 of 50 took 5.204s\n",
      "  training loss:\t\t0.375563\n",
      "  validation loss:\t\t0.402987\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 42 of 50 took 5.210s\n",
      "  training loss:\t\t0.375017\n",
      "  validation loss:\t\t0.402538\n",
      "  validation accuracy:\t\t87.86 %\n",
      "Epoch 43 of 50 took 5.227s\n",
      "  training loss:\t\t0.374914\n",
      "  validation loss:\t\t0.399931\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 44 of 50 took 5.304s\n",
      "  training loss:\t\t0.373794\n",
      "  validation loss:\t\t0.399910\n",
      "  validation accuracy:\t\t87.88 %\n",
      "Epoch 45 of 50 took 5.225s\n",
      "  training loss:\t\t0.373540\n",
      "  validation loss:\t\t0.403777\n",
      "  validation accuracy:\t\t87.88 %\n",
      "Epoch 46 of 50 took 5.365s\n",
      "  training loss:\t\t0.373206\n",
      "  validation loss:\t\t0.400826\n",
      "  validation accuracy:\t\t87.83 %\n",
      "Epoch 47 of 50 took 5.239s\n",
      "  training loss:\t\t0.371927\n",
      "  validation loss:\t\t0.403487\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 48 of 50 took 5.470s\n",
      "  training loss:\t\t0.371421\n",
      "  validation loss:\t\t0.406131\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 49 of 50 took 5.260s\n",
      "  training loss:\t\t0.370551\n",
      "  validation loss:\t\t0.404825\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 50 of 50 took 5.294s\n",
      "  training loss:\t\t0.369641\n",
      "  validation loss:\t\t0.404667\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Final results:\n",
      "  test loss:\t\t\t0.435920\n",
      "  test accuracy:\t\t87.16 %\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "\n",
    "# ################## Download and prepare the MNIST dataset ##################\n",
    "# This is just some way of getting the MNIST dataset from an online location\n",
    "# and loading it into numpy arrays. It doesn't involve Lasagne at all.\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    npdata = np.load('data3/Accidents_allyear_numpy_standardised2.npy')\n",
    "    npdata = npdata.astype(np.float32)\n",
    "    X_train, X_val = npdata[0:29951, 0:53].reshape(29951,1,53), npdata[29951:35576, 0:53].reshape(5625,1,53)\n",
    "    y_train, y_val = npdata[0:29951, 53]-1, npdata[29951:35576, 53]-1\n",
    "    x_test, y_test = npdata[35576:41113, 0:53].reshape(5537,1,53), npdata[35576:41113, 53]-1\n",
    "    print (X_train.shape, y_train.shape, X_val.shape, y_val.shape, npdata.shape)\n",
    "    return X_train, y_train, X_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "# ##################### Build the neural network model #######################\n",
    "# This script supports three types of models. For each one, we define a\n",
    "# function that takes a Theano variable representing the input and returns\n",
    "# the output layer of a neural network model built in Lasagne.\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "    # a softmax output layer of 10 units. It applies 20% dropout to the input\n",
    "    # data and 50% dropout to the hidden layers.\n",
    "\n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    # (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
    "    # linking it to the given Theano variable `input_var`, if any:\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None,1,53),\n",
    "                                     input_var=input_var)\n",
    "\n",
    "    # Apply 20% dropout to the input data:\n",
    "    #l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "\n",
    "    # Add a fully-connected layer of 800 units, using the linear rectifier, and\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=300,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # We'll now add dropout of 50%:\n",
    "    #l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    # Another 800-unit layer:\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1, num_units=400,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "#     l_hid3 = lasagne.layers.DenseLayer(\n",
    "#             l_hid2, num_units=500,\n",
    "#             nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid2, num_units=3,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    # Each layer is linked to its incoming layer(s), so we only need to pass\n",
    "    # the output layer to give access to a network in Lasagne:\n",
    "    return l_out\n",
    "\n",
    "\n",
    "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2,\n",
    "                     drop_hidden=.5):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "                network, width, nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 10, nonlinearity=softmax)\n",
    "    return network\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "# ############################## Main program ################################\n",
    "# Everything else will be handled in our main program now. We could pull out\n",
    "# more functions to better separate the code, but it wouldn't make it any\n",
    "# easier to read.\n",
    "\n",
    "def main(model='mlp', num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor3('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    if model == 'mlp':\n",
    "        network = build_mlp(input_var)\n",
    "    elif model.startswith('custom_mlp:'):\n",
    "        depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
    "        network = build_custom_mlp(input_var, int(depth), int(width),\n",
    "                                   float(drop_in), float(drop_hid))\n",
    "    elif model == 'cnn':\n",
    "        network = build_cnn(input_var)\n",
    "    else:\n",
    "        print(\"Unrecognized model type %r.\" % model)\n",
    "        return\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 10, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 10, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if ('--help' in sys.argv) or ('-h' in sys.argv):\n",
    "        print(\"Trains a neural network on MNIST using Lasagne.\")\n",
    "        print(\"Usage: %s [MODEL [EPOCHS]]\" % sys.argv[0])\n",
    "        print()\n",
    "        print(\"MODEL: 'mlp' for a simple Multi-Layer Perceptron (MLP),\")\n",
    "        print(\"       'custom_mlp:DEPTH,WIDTH,DROP_IN,DROP_HID' for an MLP\")\n",
    "        print(\"       with DEPTH hidden layers of WIDTH units, DROP_IN\")\n",
    "        print(\"       input dropout and DROP_HID hidden dropout,\")\n",
    "        print(\"       'cnn' for a simple Convolutional Neural Network (CNN).\")\n",
    "        print(\"EPOCHS: number of training epochs to perform (default: 500)\")\n",
    "    else:\n",
    "        kwargs = {}\n",
    "        if len(sys.argv) > 1:\n",
    "            kwargs['model'] = 'mlp'\n",
    "        if len(sys.argv) > 2:\n",
    "            kwargs['num_epochs'] = 50\n",
    "        main(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    36170\n",
       "2.0     4264\n",
       "1.0      679\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('data3/Accidents_allyear_numpy_standardised2.npy')\n",
    "a = pd.Series(data[:,53])\n",
    "\n",
    "a.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8797703889280762"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36170.0/(36170+4264+679)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     36170\n",
       "1.0      4307\n",
       "2.0       466\n",
       "3.0       102\n",
       "4.0        37\n",
       "6.0        11\n",
       "5.0        11\n",
       "7.0         4\n",
       "8.0         2\n",
       "19.0        1\n",
       "18.0        1\n",
       "9.0         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('data3/Accidents_allyear_numpy_standardised2.npy')\n",
    "a = pd.Series(data[:,54])\n",
    "a.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
