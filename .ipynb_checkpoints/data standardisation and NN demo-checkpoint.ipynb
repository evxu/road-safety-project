{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/XueqiWang/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/Users/XueqiWang/anaconda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import time\n",
    "import math\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "a = '24/7/2016'\n",
    "a = time.strptime(a,\"%d/%m/%Y\")\n",
    "type(a)\n",
    "print time.strftime('%w',a) # Weekday as a decimal number, where 0 is Sunday and 6 is Saturday.\n",
    "print time.strftime('%W',a) # Week number of the year (Monday as the first day of the week) \n",
    "                            # as a decimal number [00,53]. \n",
    "                            # All days in a new year preceding the first Monday are considered to \n",
    "                            # be in week 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment on 2014 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def one_hot_code(encode, column, start, npdata):\n",
    "    rawdata = df[column].values\n",
    "    i = 0\n",
    "    for j in rawdata:\n",
    "        npdata[i, start + encode[j]] = 1\n",
    "        i+=1\n",
    "\n",
    "\n",
    "df = pd.read_csv('data2/Accidents_kis_2014.csv')\n",
    "## select features\n",
    "df = df[['Number_of_Vehicles', 'Date', 'Time','Longitude', \n",
    "         'Latitude', 'Speed_limit', 'Junction_Detail', 'Junction_Control', 'Light_Conditions', \n",
    "         'Weather_Conditions','Road_Surface_Conditions', 'Special_Conditions_at_Site', \n",
    "         'Urban_or_Rural_Area', 'Accident_Severity','Number_of_KIS']]\n",
    "\n",
    "## drop missing data\n",
    "# weather_Conditions\n",
    "# Road_Surface_Conditions\n",
    "df = df.drop(df[df.Road_Surface_Conditions < 0].index)\n",
    "# Special_Conditions_at_site\n",
    "# Urban_or_Rural_Area\n",
    "\n",
    "## Create numpy array\n",
    "npdata = np.zeros(shape=(len(df),57))  # features(0-52)|Accident_severity(53-55)|number_of_kis\n",
    "\n",
    "## number of vehicles\n",
    "npdata[:,0] = df['Number_of_Vehicles'].values\n",
    "\n",
    "## month, week, weekday\n",
    "day = df['Date'].values\n",
    "i = 0\n",
    "for d in day:\n",
    "    d = time.strptime(d, '%d/%m/%Y')\n",
    "    npdata[i,1] = time.strftime('%m', d) # month\n",
    "    npdata[i,2] = time.strftime('%W', d) # week\n",
    "    npdata[i,3] = time.strftime('%w', d) # weekday\n",
    "    i += 1\n",
    "    \n",
    "## time1 and time2 (sin and cos)\n",
    "tt = df['Time'].values\n",
    "i = 0\n",
    "for t in tt:\n",
    "    hm = t.split(':')\n",
    "    t = float(hm[0])*60 + float(hm[1])\n",
    "    npdata[i,4] = math.sin(2* math.pi * t/1440)\n",
    "    npdata[i,5] = math.cos(2* math.pi * t/1440)\n",
    "    \n",
    "## longitude and latitude\n",
    "npdata[:,6] = df['Longitude'].values\n",
    "npdata[:,7] = df['Latitude'].values\n",
    "\n",
    "## speed limit\n",
    "npdata[:,8] = df['Speed_limit'].values\n",
    "\n",
    "## Junction Detail\n",
    "encode = {0: 0, 1:1, 2: 2, 3:3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "one_hot_code(encode = encode, column = 'Junction_Detail', start = 9, npdata = npdata)\n",
    "\n",
    "## Junction Control\n",
    "junc = df['Junction_Control'].values\n",
    "i = 0\n",
    "for j in junc:\n",
    "    if j == -1:\n",
    "        npdata[i,18] = 1\n",
    "    else:\n",
    "        npdata[i, 18+j] = 1\n",
    "    i+=1\n",
    "\n",
    "## Light Conditions\n",
    "encode = {1:0, 4:1, 5:2, 6:3, 7:4}\n",
    "one_hot_code(encode = encode, column = 'Light_Conditions', start = 23, npdata = npdata)\n",
    "\n",
    "## Weather Conditions\n",
    "encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "one_hot_code(encode = encode, column = 'Weather_Conditions', start = 28, npdata = npdata)\n",
    "\n",
    "## Road surface conditions\n",
    "encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6}\n",
    "one_hot_code(encode = encode, column = 'Road_Surface_Conditions', start = 37, npdata = npdata)\n",
    "\n",
    "## Special conditions at site\n",
    "encode = {0:0, 1: 1, 2: 2, 3: 3, 4:4, 5:5, 6:6, 7:7}\n",
    "one_hot_code(encode = encode, column = 'Special_Conditions_at_Site', start = 44, npdata = npdata)\n",
    "\n",
    "## urban or rural\n",
    "ur = df['Urban_or_Rural_Area'].values\n",
    "i = 0\n",
    "for j in ur:\n",
    "    if j==1:\n",
    "        npdata[i, 52] = 1\n",
    "    else:\n",
    "        npdata[i, 52] = -1\n",
    "    i+=1\n",
    "\n",
    "## Accident severity\n",
    "encode = {1:0, 2:1, 3:2}\n",
    "one_hot_code(encode = encode, column = 'Accident_Severity', start = 53, npdata = npdata)\n",
    "\n",
    "## Number of KIS\n",
    "npdata[:,56] = df['Number_of_KIS'].values\n",
    "\n",
    "np.save('data2/demo_2014_numpy_encoded.npy', npdata) # implement encoding, wait for normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npdata = np.load('data2/demo_2014_numpy_encoded.npy')\n",
    "clm = [0, 1, 2, 3,6,7,8]\n",
    "for c in clm:\n",
    "    norm = npdata[:,c]\n",
    "    norm = (norm - np.mean(norm))/np.std(norm)\n",
    "    npdata[:,c] = norm\n",
    "\n",
    "np.save('data2/demo_2014_numpy_standardised.npy', npdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Neural network trail - 2014 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5625, 57)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(npdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 100 took 0.092s\n",
      "  training loss:\t\t0.191360\n",
      "  validation loss:\t\t0.241574\n",
      "Epoch 2 of 100 took 0.091s\n",
      "  training loss:\t\t0.179982\n",
      "  validation loss:\t\t0.239769\n",
      "Epoch 3 of 100 took 0.106s\n",
      "  training loss:\t\t0.177733\n",
      "  validation loss:\t\t0.237417\n",
      "Epoch 4 of 100 took 0.093s\n",
      "  training loss:\t\t0.173950\n",
      "  validation loss:\t\t0.237353\n",
      "Epoch 5 of 100 took 0.091s\n",
      "  training loss:\t\t0.172813\n",
      "  validation loss:\t\t0.243403\n",
      "Epoch 6 of 100 took 0.100s\n",
      "  training loss:\t\t0.169708\n",
      "  validation loss:\t\t0.242203\n",
      "Epoch 7 of 100 took 0.091s\n",
      "  training loss:\t\t0.168837\n",
      "  validation loss:\t\t0.241939\n",
      "Epoch 8 of 100 took 0.091s\n",
      "  training loss:\t\t0.166193\n",
      "  validation loss:\t\t0.254769\n",
      "Epoch 9 of 100 took 0.103s\n",
      "  training loss:\t\t0.165298\n",
      "  validation loss:\t\t0.242451\n",
      "Epoch 10 of 100 took 0.092s\n",
      "  training loss:\t\t0.162526\n",
      "  validation loss:\t\t0.250024\n",
      "Epoch 11 of 100 took 0.092s\n",
      "  training loss:\t\t0.160275\n",
      "  validation loss:\t\t0.243868\n",
      "Epoch 12 of 100 took 0.100s\n",
      "  training loss:\t\t0.158513\n",
      "  validation loss:\t\t0.250748\n",
      "Epoch 13 of 100 took 0.092s\n",
      "  training loss:\t\t0.155472\n",
      "  validation loss:\t\t0.246490\n",
      "Epoch 14 of 100 took 0.090s\n",
      "  training loss:\t\t0.151632\n",
      "  validation loss:\t\t0.255186\n",
      "Epoch 15 of 100 took 0.098s\n",
      "  training loss:\t\t0.149890\n",
      "  validation loss:\t\t0.276309\n",
      "Epoch 16 of 100 took 0.097s\n",
      "  training loss:\t\t0.153191\n",
      "  validation loss:\t\t0.261735\n",
      "Epoch 17 of 100 took 0.091s\n",
      "  training loss:\t\t0.149229\n",
      "  validation loss:\t\t0.252219\n",
      "Epoch 18 of 100 took 0.104s\n",
      "  training loss:\t\t0.148753\n",
      "  validation loss:\t\t0.266246\n",
      "Epoch 19 of 100 took 0.095s\n",
      "  training loss:\t\t0.146889\n",
      "  validation loss:\t\t0.298769\n",
      "Epoch 20 of 100 took 0.093s\n",
      "  training loss:\t\t0.152799\n",
      "  validation loss:\t\t0.293441\n",
      "Epoch 21 of 100 took 0.108s\n",
      "  training loss:\t\t0.146918\n",
      "  validation loss:\t\t0.275368\n",
      "Epoch 22 of 100 took 0.111s\n",
      "  training loss:\t\t0.144719\n",
      "  validation loss:\t\t0.294639\n",
      "Epoch 23 of 100 took 0.109s\n",
      "  training loss:\t\t0.142373\n",
      "  validation loss:\t\t0.247021\n",
      "Epoch 24 of 100 took 0.113s\n",
      "  training loss:\t\t0.142562\n",
      "  validation loss:\t\t0.267436\n",
      "Epoch 25 of 100 took 0.108s\n",
      "  training loss:\t\t0.142096\n",
      "  validation loss:\t\t0.263469\n",
      "Epoch 26 of 100 took 0.108s\n",
      "  training loss:\t\t0.138096\n",
      "  validation loss:\t\t0.270943\n",
      "Epoch 27 of 100 took 0.118s\n",
      "  training loss:\t\t0.136778\n",
      "  validation loss:\t\t0.321970\n",
      "Epoch 28 of 100 took 0.155s\n",
      "  training loss:\t\t0.137068\n",
      "  validation loss:\t\t0.326108\n",
      "Epoch 29 of 100 took 0.151s\n",
      "  training loss:\t\t0.133401\n",
      "  validation loss:\t\t0.260366\n",
      "Epoch 30 of 100 took 0.128s\n",
      "  training loss:\t\t0.137877\n",
      "  validation loss:\t\t0.259979\n",
      "Epoch 31 of 100 took 0.136s\n",
      "  training loss:\t\t0.130110\n",
      "  validation loss:\t\t0.276990\n",
      "Epoch 32 of 100 took 0.155s\n",
      "  training loss:\t\t0.131149\n",
      "  validation loss:\t\t0.284808\n",
      "Epoch 33 of 100 took 0.160s\n",
      "  training loss:\t\t0.129563\n",
      "  validation loss:\t\t0.330257\n",
      "Epoch 34 of 100 took 0.142s\n",
      "  training loss:\t\t0.128464\n",
      "  validation loss:\t\t0.475403\n",
      "Epoch 35 of 100 took 0.110s\n",
      "  training loss:\t\t0.126518\n",
      "  validation loss:\t\t0.263898\n",
      "Epoch 36 of 100 took 0.143s\n",
      "  training loss:\t\t0.126937\n",
      "  validation loss:\t\t0.292271\n",
      "Epoch 37 of 100 took 0.162s\n",
      "  training loss:\t\t0.123691\n",
      "  validation loss:\t\t0.267398\n",
      "Epoch 38 of 100 took 0.149s\n",
      "  training loss:\t\t0.125244\n",
      "  validation loss:\t\t0.284246\n",
      "Epoch 39 of 100 took 0.129s\n",
      "  training loss:\t\t0.130462\n",
      "  validation loss:\t\t0.281023\n",
      "Epoch 40 of 100 took 0.138s\n",
      "  training loss:\t\t0.125998\n",
      "  validation loss:\t\t0.269433\n",
      "Epoch 41 of 100 took 0.151s\n",
      "  training loss:\t\t0.124435\n",
      "  validation loss:\t\t0.299496\n",
      "Epoch 42 of 100 took 0.121s\n",
      "  training loss:\t\t0.118888\n",
      "  validation loss:\t\t0.286849\n",
      "Epoch 43 of 100 took 0.113s\n",
      "  training loss:\t\t0.122647\n",
      "  validation loss:\t\t0.517749\n",
      "Epoch 44 of 100 took 0.134s\n",
      "  training loss:\t\t0.120978\n",
      "  validation loss:\t\t0.286752\n",
      "Epoch 45 of 100 took 0.122s\n",
      "  training loss:\t\t0.119264\n",
      "  validation loss:\t\t0.295382\n",
      "Epoch 46 of 100 took 0.123s\n",
      "  training loss:\t\t0.125332\n",
      "  validation loss:\t\t0.272303\n",
      "Epoch 47 of 100 took 0.141s\n",
      "  training loss:\t\t0.125885\n",
      "  validation loss:\t\t0.276659\n",
      "Epoch 48 of 100 took 0.112s\n",
      "  training loss:\t\t0.120255\n",
      "  validation loss:\t\t0.287225\n",
      "Epoch 49 of 100 took 0.115s\n",
      "  training loss:\t\t0.115503\n",
      "  validation loss:\t\t0.266446\n",
      "Epoch 50 of 100 took 0.114s\n",
      "  training loss:\t\t0.115149\n",
      "  validation loss:\t\t0.291670\n",
      "Epoch 51 of 100 took 0.118s\n",
      "  training loss:\t\t0.115348\n",
      "  validation loss:\t\t0.323164\n",
      "Epoch 52 of 100 took 0.120s\n",
      "  training loss:\t\t0.116273\n",
      "  validation loss:\t\t0.315071\n",
      "Epoch 53 of 100 took 0.109s\n",
      "  training loss:\t\t0.113394\n",
      "  validation loss:\t\t0.312534\n",
      "Epoch 54 of 100 took 0.104s\n",
      "  training loss:\t\t0.110723\n",
      "  validation loss:\t\t0.476076\n",
      "Epoch 55 of 100 took 0.116s\n",
      "  training loss:\t\t0.112287\n",
      "  validation loss:\t\t0.286778\n",
      "Epoch 56 of 100 took 0.127s\n",
      "  training loss:\t\t0.117368\n",
      "  validation loss:\t\t0.287848\n",
      "Epoch 57 of 100 took 0.123s\n",
      "  training loss:\t\t0.113806\n",
      "  validation loss:\t\t0.317163\n",
      "Epoch 58 of 100 took 0.126s\n",
      "  training loss:\t\t0.111922\n",
      "  validation loss:\t\t0.270225\n",
      "Epoch 59 of 100 took 0.134s\n",
      "  training loss:\t\t0.108743\n",
      "  validation loss:\t\t0.276934\n",
      "Epoch 60 of 100 took 0.103s\n",
      "  training loss:\t\t0.106201\n",
      "  validation loss:\t\t0.280749\n",
      "Epoch 61 of 100 took 0.109s\n",
      "  training loss:\t\t0.105853\n",
      "  validation loss:\t\t0.285127\n",
      "Epoch 62 of 100 took 0.137s\n",
      "  training loss:\t\t0.107708\n",
      "  validation loss:\t\t0.293535\n",
      "Epoch 63 of 100 took 0.115s\n",
      "  training loss:\t\t0.107845\n",
      "  validation loss:\t\t0.302446\n",
      "Epoch 64 of 100 took 0.137s\n",
      "  training loss:\t\t0.105548\n",
      "  validation loss:\t\t0.310508\n",
      "Epoch 65 of 100 took 0.171s\n",
      "  training loss:\t\t0.102989\n",
      "  validation loss:\t\t0.396001\n",
      "Epoch 66 of 100 took 0.136s\n",
      "  training loss:\t\t0.105607\n",
      "  validation loss:\t\t0.342014\n",
      "Epoch 67 of 100 took 0.151s\n",
      "  training loss:\t\t0.099623\n",
      "  validation loss:\t\t0.312706\n",
      "Epoch 68 of 100 took 0.142s\n",
      "  training loss:\t\t0.100788\n",
      "  validation loss:\t\t0.559424\n",
      "Epoch 69 of 100 took 0.134s\n",
      "  training loss:\t\t0.101196\n",
      "  validation loss:\t\t0.269046\n",
      "Epoch 70 of 100 took 0.124s\n",
      "  training loss:\t\t0.097521\n",
      "  validation loss:\t\t0.294049\n",
      "Epoch 71 of 100 took 0.110s\n",
      "  training loss:\t\t0.101677\n",
      "  validation loss:\t\t0.370122\n",
      "Epoch 72 of 100 took 0.164s\n",
      "  training loss:\t\t0.095798\n",
      "  validation loss:\t\t0.288921\n",
      "Epoch 73 of 100 took 0.163s\n",
      "  training loss:\t\t0.103783\n",
      "  validation loss:\t\t0.342819\n",
      "Epoch 74 of 100 took 0.152s\n",
      "  training loss:\t\t0.093023\n",
      "  validation loss:\t\t0.376945\n",
      "Epoch 75 of 100 took 0.114s\n",
      "  training loss:\t\t0.098032\n",
      "  validation loss:\t\t0.335569\n",
      "Epoch 76 of 100 took 0.104s\n",
      "  training loss:\t\t0.120020\n",
      "  validation loss:\t\t0.281105\n",
      "Epoch 77 of 100 took 0.107s\n",
      "  training loss:\t\t0.106336\n",
      "  validation loss:\t\t0.280630\n",
      "Epoch 78 of 100 took 0.111s\n",
      "  training loss:\t\t0.107566\n",
      "  validation loss:\t\t0.281838\n",
      "Epoch 79 of 100 took 0.107s\n",
      "  training loss:\t\t0.100669\n",
      "  validation loss:\t\t0.300206\n",
      "Epoch 80 of 100 took 0.124s\n",
      "  training loss:\t\t0.098274\n",
      "  validation loss:\t\t0.282256\n",
      "Epoch 81 of 100 took 0.124s\n",
      "  training loss:\t\t0.095461\n",
      "  validation loss:\t\t0.283181\n",
      "Epoch 82 of 100 took 0.105s\n",
      "  training loss:\t\t0.092788\n",
      "  validation loss:\t\t0.312414\n",
      "Epoch 83 of 100 took 0.102s\n",
      "  training loss:\t\t0.095370\n",
      "  validation loss:\t\t0.290758\n",
      "Epoch 84 of 100 took 0.112s\n",
      "  training loss:\t\t0.094477\n",
      "  validation loss:\t\t0.314039\n",
      "Epoch 85 of 100 took 0.105s\n",
      "  training loss:\t\t0.091820\n",
      "  validation loss:\t\t0.320169\n",
      "Epoch 86 of 100 took 0.102s\n",
      "  training loss:\t\t0.089080\n",
      "  validation loss:\t\t0.296136\n",
      "Epoch 87 of 100 took 0.109s\n",
      "  training loss:\t\t0.088262\n",
      "  validation loss:\t\t0.356387\n",
      "Epoch 88 of 100 took 0.106s\n",
      "  training loss:\t\t0.086716\n",
      "  validation loss:\t\t0.390722\n",
      "Epoch 89 of 100 took 0.104s\n",
      "  training loss:\t\t0.098461\n",
      "  validation loss:\t\t0.317295\n",
      "Epoch 90 of 100 took 0.112s\n",
      "  training loss:\t\t0.092690\n",
      "  validation loss:\t\t0.293122\n",
      "Epoch 91 of 100 took 0.104s\n",
      "  training loss:\t\t0.087708\n",
      "  validation loss:\t\t0.379016\n",
      "Epoch 92 of 100 took 0.117s\n",
      "  training loss:\t\t0.095240\n",
      "  validation loss:\t\t0.310450\n",
      "Epoch 93 of 100 took 0.103s\n",
      "  training loss:\t\t0.094383\n",
      "  validation loss:\t\t0.280790\n",
      "Epoch 94 of 100 took 0.113s\n",
      "  training loss:\t\t0.087775\n",
      "  validation loss:\t\t0.340009\n",
      "Epoch 95 of 100 took 0.105s\n",
      "  training loss:\t\t0.091108\n",
      "  validation loss:\t\t0.274870\n",
      "Epoch 96 of 100 took 0.103s\n",
      "  training loss:\t\t0.088719\n",
      "  validation loss:\t\t0.435992\n",
      "Epoch 97 of 100 took 0.109s\n",
      "  training loss:\t\t0.086180\n",
      "  validation loss:\t\t0.362796\n",
      "Epoch 98 of 100 took 0.105s\n",
      "  training loss:\t\t0.086014\n",
      "  validation loss:\t\t0.293281\n",
      "Epoch 99 of 100 took 0.110s\n",
      "  training loss:\t\t0.084016\n",
      "  validation loss:\t\t0.292101\n",
      "Epoch 100 of 100 took 0.104s\n",
      "  training loss:\t\t0.086418\n",
      "  validation loss:\t\t0.294459\n",
      "Final results:\n",
      "  test loss:\t\t\t0.430685\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "def load_dataset():\n",
    "    npdata = np.load('data2/demo_2014_numpy_standardised.npy')\n",
    "#     (m,n) = np.shape(npdata)\n",
    "#     npdata = npdata.reshape((m,1,n))\n",
    "    npdata = npdata.astype(np.float32)\n",
    "    X_train, X_val = npdata[0:4000, 0:53].reshape(4000,1,53), npdata[4000:4800, 0:53].reshape(800,1,53)\n",
    "    y_train, y_val = npdata[0:4000, 56].reshape(4000,1), npdata[4000:4800, 56].reshape(800,1)\n",
    "    x_test, y_test = npdata[4800:5625, 0:53].reshape(825,1,53), npdata[4800:5625, 56].reshape(825,1)\n",
    "    #print (X_train.shape, y_train.shape, X_val.shape, y_val.shape, npdata.shape)\n",
    "    return X_train, y_train, X_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "# ##################### Build the neural network model #######################\n",
    "# This script supports three types of models. For each one, we define a\n",
    "# function that takes a Theano variable representing the input and returns\n",
    "# the output layer of a neural network model built in Lasagne.\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "    # a softmax output layer of 10 units. It applies 20% dropout to the input\n",
    "    # data and 50% dropout to the hidden layers.\n",
    "\n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    # (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
    "    # linking it to the given Theano variable `input_var`, if any:\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None,1,53),\n",
    "                                     input_var=input_var)\n",
    "\n",
    "    # Apply 20% dropout to the input data:\n",
    "    #l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "\n",
    "    # Add a fully-connected layer of 800 units, using the linear rectifier, and\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # We'll now add dropout of 50%:\n",
    "    #l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    # Another 800-unit layer:\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # 50% dropout again:\n",
    "    #l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "\n",
    "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid2, num_units=1,\n",
    "            nonlinearity=lasagne.nonlinearities.linear)\n",
    "\n",
    "    # Each layer is linked to its incoming layer(s), so we only need to pass\n",
    "    # the output layer to give access to a network in Lasagne:\n",
    "    return l_out\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "# ############################## Main program ################################\n",
    "# Everything else will be handled in our main program now. We could pull out\n",
    "# more functions to better separate the code, but it wouldn't make it any\n",
    "# easier to read.\n",
    "\n",
    "def main(model='mlp', num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor3('inputs')\n",
    "    target_var = T.matrix('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    if model == 'mlp':\n",
    "        network = build_mlp(input_var)\n",
    "    elif model.startswith('custom_mlp:'):\n",
    "        depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
    "        network = build_custom_mlp(input_var, int(depth), int(width),\n",
    "                                   float(drop_in), float(drop_hid))\n",
    "    elif model == 'cnn':\n",
    "        network = build_cnn(input_var)\n",
    "    else:\n",
    "        print(\"Unrecognized model type %r.\" % model)\n",
    "        return\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.squared_error(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    # no accuracy for regression\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast = True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], test_loss, allow_input_downcast = True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 10, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, y_val.shape[0], shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            #print (inputs.shape, targets.shape)\n",
    "            err = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        \n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if ('--help' in sys.argv) or ('-h' in sys.argv):\n",
    "        print(\"Trains a neural network on MNIST using Lasagne.\")\n",
    "        print(\"Usage: %s [MODEL [EPOCHS]]\" % sys.argv[0])\n",
    "        print()\n",
    "        print(\"MODEL: 'mlp' for a simple Multi-Layer Perceptron (MLP),\")\n",
    "        print(\"       'custom_mlp:DEPTH,WIDTH,DROP_IN,DROP_HID' for an MLP\")\n",
    "        print(\"       with DEPTH hidden layers of WIDTH units, DROP_IN\")\n",
    "        print(\"       input dropout and DROP_HID hidden dropout,\")\n",
    "        print(\"       'cnn' for a simple Convolutional Neural Network (CNN).\")\n",
    "        print(\"EPOCHS: number of training epochs to perform (default: 500)\")\n",
    "    else:\n",
    "        kwargs = {}\n",
    "        if len(sys.argv) > 1:\n",
    "            kwargs['model'] = 'mlp'\n",
    "        if len(sys.argv) > 2:\n",
    "            kwargs['num_epochs'] = 100\n",
    "        main(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data standardisation - the rest of the years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data cleaning - remove missing data in each year\n",
    "2. Data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in 2009:  6638\n",
      "The number of samples in 2010:  6499\n",
      "The number of samples in 2011:  5819\n",
      "The number of samples in 2012:  5607\n",
      "The number of samples in 2013:  5388\n",
      "The number of samples in 2014:  5625\n",
      "The number of samples in 2015:  5537\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def one_hot_code(encode, column, start, npdata):\n",
    "    rawdata = df[column].values\n",
    "    i = 0\n",
    "    for j in rawdata:\n",
    "        npdata[i, start + encode[j]] = 1\n",
    "        i+=1\n",
    "\n",
    "years = ['2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "for year in years:\n",
    "    df = pd.read_csv('data2/Accidents_kis_' + year + '.csv')\n",
    "    ## select features\n",
    "    df = df[['Number_of_Vehicles', 'Date', 'Time','Longitude', \n",
    "             'Latitude', 'Speed_limit', 'Junction_Detail', 'Junction_Control', 'Light_Conditions', \n",
    "             'Weather_Conditions','Road_Surface_Conditions', 'Special_Conditions_at_Site', \n",
    "             'Urban_or_Rural_Area', 'Accident_Severity','Number_of_KIS']]\n",
    "\n",
    "    ## drop missing data\n",
    "    # weather_Conditions\n",
    "    df = df.drop(df[df.Weather_Conditions < 0].index)\n",
    "    # Road_Surface_Conditions\n",
    "    df = df.drop(df[df.Road_Surface_Conditions < 0].index)\n",
    "    # Special_Conditions_at_site\n",
    "    df = df.drop(df[df.Special_Conditions_at_Site < 0].index)\n",
    "    # Urban_or_Rural_Area\n",
    "    df = df.drop(df[df.Urban_or_Rural_Area < 0].index)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print ('The number of samples in '+year+ ': ', len(df))\n",
    "    ## Create numpy array\n",
    "    npdata = np.zeros(shape=(len(df),57))  # features(0-52)|Accident_severity(53-55)|number_of_kis\n",
    "\n",
    "    ## number of vehicles\n",
    "    npdata[:,0] = df['Number_of_Vehicles'].values\n",
    "\n",
    "    ## month, week, weekday\n",
    "    day = df['Date'].values\n",
    "    i = 0\n",
    "    for d in day:\n",
    "        d = time.strptime(d, '%d/%m/%Y')\n",
    "        npdata[i,1] = time.strftime('%m', d) # month\n",
    "        npdata[i,2] = time.strftime('%W', d) # week\n",
    "        npdata[i,3] = time.strftime('%w', d) # weekday\n",
    "        i += 1\n",
    "\n",
    "    ## time1 and time2 (sin and cos)\n",
    "    tt = df['Time'].values\n",
    "    i = 0\n",
    "    for t in tt:\n",
    "        hm = t.split(':')\n",
    "        t = float(hm[0])*60 + float(hm[1])\n",
    "        npdata[i,4] = math.sin(2* math.pi * t/1440)\n",
    "        npdata[i,5] = math.cos(2* math.pi * t/1440)\n",
    "\n",
    "    ## longitude and latitude\n",
    "    npdata[:,6] = df['Longitude'].values\n",
    "    npdata[:,7] = df['Latitude'].values\n",
    "\n",
    "    ## speed limit\n",
    "    npdata[:,8] = df['Speed_limit'].values\n",
    "\n",
    "    ## Junction Detail\n",
    "    encode = {0: 0, 1:1, 2: 2, 3:3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "    one_hot_code(encode = encode, column = 'Junction_Detail', start = 9, npdata = npdata)\n",
    "\n",
    "    ## Junction Control\n",
    "    junc = df['Junction_Control'].values\n",
    "    i = 0\n",
    "    for j in junc:\n",
    "        if j == -1:\n",
    "            npdata[i,18] = 1\n",
    "        else:\n",
    "            npdata[i, 18+j] = 1\n",
    "        i+=1\n",
    "\n",
    "    ## Light Conditions\n",
    "    encode = {1:0, 4:1, 5:2, 6:3, 7:4}\n",
    "    one_hot_code(encode = encode, column = 'Light_Conditions', start = 23, npdata = npdata)\n",
    "\n",
    "    ## Weather Conditions\n",
    "    encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "    one_hot_code(encode = encode, column = 'Weather_Conditions', start = 28, npdata = npdata)\n",
    "\n",
    "    ## Road surface conditions\n",
    "    encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6}\n",
    "    one_hot_code(encode = encode, column = 'Road_Surface_Conditions', start = 37, npdata = npdata)\n",
    "\n",
    "    ## Special conditions at site\n",
    "    encode = {0:0, 1: 1, 2: 2, 3: 3, 4:4, 5:5, 6:6, 7:7}\n",
    "    one_hot_code(encode = encode, column = 'Special_Conditions_at_Site', start = 44, npdata = npdata)\n",
    "\n",
    "    ## urban or rural\n",
    "    ur = df['Urban_or_Rural_Area'].values\n",
    "    i = 0\n",
    "    for j in ur:\n",
    "        if j==1:\n",
    "            npdata[i, 52] = 1\n",
    "        else:\n",
    "            npdata[i, 52] = -1\n",
    "        i+=1\n",
    "\n",
    "    ## Accident severity\n",
    "    encode = {1:0, 2:1, 3:2}\n",
    "    one_hot_code(encode = encode, column = 'Accident_Severity', start = 53, npdata = npdata)\n",
    "\n",
    "    ## Number of KIS\n",
    "    npdata[:,56] = df['Number_of_KIS'].values\n",
    "\n",
    "    np.save('data3/Accidents_'+ year + '_numpy_encoded.npy', npdata) # save each year's data as numpy array\n",
    "    #implement encoding, wait for normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009: (6638, 57)\n",
      "2010: (6499, 57)\n",
      "2011: (5819, 57)\n",
      "2012: (5607, 57)\n",
      "2013: (5388, 57)\n",
      "2014: (5625, 57)\n",
      "2015: (5537, 57)\n",
      "all data: (41113, 57)\n"
     ]
    }
   ],
   "source": [
    "npdata = np.load('data3/Accidents_2009_numpy_encoded.npy') # initialise numpy array\n",
    "print ('2009:', npdata.shape)\n",
    "years = ['2010', '2011', '2012', '2013', '2014', '2015']\n",
    "for year in years:\n",
    "    newdata = np.load('data3/Accidents_' + year + '_numpy_encoded.npy')\n",
    "    print (year+':', newdata.shape)\n",
    "    npdata = np.vstack([npdata,newdata])\n",
    "    \n",
    "print ('all data:', npdata.shape)\n",
    "\n",
    "clm = [0, 1, 2, 3,6,7,8]\n",
    "for c in clm:\n",
    "    norm = npdata[:,c]\n",
    "    norm = (norm - np.mean(norm))/np.std(norm)\n",
    "    npdata[:,c] = norm\n",
    "\n",
    "np.save('data3/Accidents_allyear_numpy_standardised.npy', npdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - 1\n",
    "- predict the number of KIS\n",
    "- training data: 2009-2013\n",
    "- validaton data: 2014\n",
    "- test data: 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 100 took 11.231s\n",
      "  training loss:\t\t0.212666\n",
      "  validation loss:\t\t0.192709\n",
      "Epoch 2 of 100 took 12.162s\n",
      "  training loss:\t\t0.209046\n",
      "  validation loss:\t\t0.191361\n",
      "Epoch 3 of 100 took 10.831s\n",
      "  training loss:\t\t0.207228\n",
      "  validation loss:\t\t0.191127\n",
      "Epoch 4 of 100 took 10.953s\n",
      "  training loss:\t\t0.206416\n",
      "  validation loss:\t\t0.190783\n",
      "Epoch 5 of 100 took 13.720s\n",
      "  training loss:\t\t0.205659\n",
      "  validation loss:\t\t0.191697\n",
      "Epoch 6 of 100 took 12.095s\n",
      "  training loss:\t\t0.205206\n",
      "  validation loss:\t\t0.191024\n",
      "Epoch 7 of 100 took 12.112s\n",
      "  training loss:\t\t0.204852\n",
      "  validation loss:\t\t0.190848\n",
      "Epoch 8 of 100 took 13.609s\n",
      "  training loss:\t\t0.204485\n",
      "  validation loss:\t\t0.190995\n",
      "Epoch 9 of 100 took 13.351s\n",
      "  training loss:\t\t0.204161\n",
      "  validation loss:\t\t0.190870\n",
      "Epoch 10 of 100 took 12.117s\n",
      "  training loss:\t\t0.203737\n",
      "  validation loss:\t\t0.191027\n",
      "Epoch 11 of 100 took 12.087s\n",
      "  training loss:\t\t0.203432\n",
      "  validation loss:\t\t0.190914\n",
      "Epoch 12 of 100 took 12.187s\n",
      "  training loss:\t\t0.203224\n",
      "  validation loss:\t\t0.191075\n",
      "Epoch 13 of 100 took 12.093s\n",
      "  training loss:\t\t0.202816\n",
      "  validation loss:\t\t0.190902\n",
      "Epoch 14 of 100 took 13.455s\n",
      "  training loss:\t\t0.202648\n",
      "  validation loss:\t\t0.191032\n",
      "Epoch 15 of 100 took 12.176s\n",
      "  training loss:\t\t0.202129\n",
      "  validation loss:\t\t0.191007\n",
      "Epoch 16 of 100 took 15.678s\n",
      "  training loss:\t\t0.202177\n",
      "  validation loss:\t\t0.191865\n",
      "Epoch 17 of 100 took 13.702s\n",
      "  training loss:\t\t0.201828\n",
      "  validation loss:\t\t0.191397\n",
      "Epoch 18 of 100 took 12.404s\n",
      "  training loss:\t\t0.201661\n",
      "  validation loss:\t\t0.191384\n",
      "Epoch 19 of 100 took 12.148s\n",
      "  training loss:\t\t0.201255\n",
      "  validation loss:\t\t0.191116\n",
      "Epoch 20 of 100 took 12.102s\n",
      "  training loss:\t\t0.201186\n",
      "  validation loss:\t\t0.192246\n",
      "Epoch 21 of 100 took 13.578s\n",
      "  training loss:\t\t0.200804\n",
      "  validation loss:\t\t0.191545\n",
      "Epoch 22 of 100 took 13.124s\n",
      "  training loss:\t\t0.200803\n",
      "  validation loss:\t\t0.191378\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-bcd5a202fa77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-bcd5a202fa77>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model, num_epochs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/XueqiWang/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m                 \u001b[0;31m# TODO: provide a Param option for skipping the filter if we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0;31m#      really want speed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m                 \u001b[0;31m# see this emails for a discuation about None as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                 \u001b[0;31m# https://groups.google.com/group/theano-dev/browse_thread/thread/920a5e904e8a8525/4f1b311a28fc27e5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "def load_dataset():\n",
    "    npdata = np.load('data3/Accidents_allyear_numpy_standardised.npy')\n",
    "    npdata = npdata.astype(np.float32)\n",
    "    X_train, X_val = npdata[0:29951, 0:53].reshape(29951,1,53), npdata[29951:35576, 0:53].reshape(5625,1,53)\n",
    "    y_train, y_val = npdata[0:29951, 56].reshape(29951,1), npdata[29951:35576, 56].reshape(5625,1)\n",
    "    x_test, y_test = npdata[35576:41113, 0:53].reshape(5537,1,53), npdata[35576:41113, 56].reshape(5537,1 )\n",
    "    #print (X_train.shape, y_train.shape, X_val.shape, y_val.shape, npdata.shape)\n",
    "    return X_train, y_train, X_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "    \n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None,1,53),\n",
    "                                     input_var=input_var)\n",
    "\n",
    "    # Apply 20% dropout to the input data:\n",
    "    #l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "\n",
    "    # Add a fully-connected layer of 800 units, using the linear rectifier, and\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=300,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # We'll now add dropout of 50%:\n",
    "    #l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    # Another 800-unit layer:\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1, num_units=400,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "    l_hid3 = lasagne.layers.DenseLayer(\n",
    "            l_hid2, num_units=500,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # 50% dropout again:\n",
    "    #l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "\n",
    "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid3, num_units=1,\n",
    "            nonlinearity=lasagne.nonlinearities.linear)\n",
    "\n",
    "    # Each layer is linked to its incoming layer(s), so we only need to pass\n",
    "    # the output layer to give access to a network in Lasagne:\n",
    "    return l_out\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "# ############################## Main program ################################\n",
    "# Everything else will be handled in our main program now. We could pull out\n",
    "# more functions to better separate the code, but it wouldn't make it any\n",
    "# easier to read.\n",
    "\n",
    "def main(model='mlp', num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor3('inputs')\n",
    "    target_var = T.matrix('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    if model == 'mlp':\n",
    "        network = build_mlp(input_var)\n",
    "    elif model.startswith('custom_mlp:'):\n",
    "        depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
    "        network = build_custom_mlp(input_var, int(depth), int(width),\n",
    "                                   float(drop_in), float(drop_hid))\n",
    "    elif model == 'cnn':\n",
    "        network = build_cnn(input_var)\n",
    "    else:\n",
    "        print(\"Unrecognized model type %r.\" % model)\n",
    "        return\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.0001, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.squared_error(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    # no accuracy for regression\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast = True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], test_loss, allow_input_downcast = True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 10, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, y_val.shape[0], shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            #print (inputs.shape, targets.shape)\n",
    "            err = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        \n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if ('--help' in sys.argv) or ('-h' in sys.argv):\n",
    "        print(\"Trains a neural network on MNIST using Lasagne.\")\n",
    "        print(\"Usage: %s [MODEL [EPOCHS]]\" % sys.argv[0])\n",
    "        print()\n",
    "        print(\"MODEL: 'mlp' for a simple Multi-Layer Perceptron (MLP),\")\n",
    "        print(\"       'custom_mlp:DEPTH,WIDTH,DROP_IN,DROP_HID' for an MLP\")\n",
    "        print(\"       with DEPTH hidden layers of WIDTH units, DROP_IN\")\n",
    "        print(\"       input dropout and DROP_HID hidden dropout,\")\n",
    "        print(\"       'cnn' for a simple Convolutional Neural Network (CNN).\")\n",
    "        print(\"EPOCHS: number of training epochs to perform (default: 500)\")\n",
    "    else:\n",
    "        kwargs = {}\n",
    "        if len(sys.argv) > 1:\n",
    "            kwargs['model'] = 'mlp'\n",
    "        if len(sys.argv) > 2:\n",
    "            kwargs['num_epochs'] = 100\n",
    "        main(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not encode label of accident severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in 2009:  6638\n",
      "The number of samples in 2010:  6499\n",
      "The number of samples in 2011:  5819\n",
      "The number of samples in 2012:  5607\n",
      "The number of samples in 2013:  5388\n",
      "The number of samples in 2014:  5625\n",
      "The number of samples in 2015:  5537\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def one_hot_code(encode, column, start, npdata):\n",
    "    rawdata = df[column].values\n",
    "    i = 0\n",
    "    for j in rawdata:\n",
    "        npdata[i, start + encode[j]] = 1\n",
    "        i+=1\n",
    "\n",
    "years = ['2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "for year in years:\n",
    "    df = pd.read_csv('data2/Accidents_kis_' + year + '.csv')\n",
    "    ## select features\n",
    "    df = df[['Number_of_Vehicles', 'Date', 'Time','Longitude', \n",
    "             'Latitude', 'Speed_limit', 'Junction_Detail', 'Junction_Control', 'Light_Conditions', \n",
    "             'Weather_Conditions','Road_Surface_Conditions', 'Special_Conditions_at_Site', \n",
    "             'Urban_or_Rural_Area', 'Accident_Severity','Number_of_KIS']]\n",
    "\n",
    "    ## drop missing data\n",
    "    # weather_Conditions\n",
    "    df = df.drop(df[df.Weather_Conditions < 0].index)\n",
    "    # Road_Surface_Conditions\n",
    "    df = df.drop(df[df.Road_Surface_Conditions < 0].index)\n",
    "    # Special_Conditions_at_site\n",
    "    df = df.drop(df[df.Special_Conditions_at_Site < 0].index)\n",
    "    # Urban_or_Rural_Area\n",
    "    df = df.drop(df[df.Urban_or_Rural_Area < 0].index)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print ('The number of samples in '+year+ ': ', len(df))\n",
    "    ## Create numpy array\n",
    "    npdata = np.zeros(shape=(len(df),55))  # features(0-52)|Accident_severity(53-55)|number_of_kis\n",
    "\n",
    "    ## number of vehicles\n",
    "    npdata[:,0] = df['Number_of_Vehicles'].values\n",
    "\n",
    "    ## month, week, weekday\n",
    "    day = df['Date'].values\n",
    "    i = 0\n",
    "    for d in day:\n",
    "        d = time.strptime(d, '%d/%m/%Y')\n",
    "        npdata[i,1] = time.strftime('%m', d) # month\n",
    "        npdata[i,2] = time.strftime('%W', d) # week\n",
    "        npdata[i,3] = time.strftime('%w', d) # weekday\n",
    "        i += 1\n",
    "\n",
    "    ## time1 and time2 (sin and cos)\n",
    "    tt = df['Time'].values\n",
    "    i = 0\n",
    "    for t in tt:\n",
    "        hm = t.split(':')\n",
    "        t = float(hm[0])*60 + float(hm[1])\n",
    "        npdata[i,4] = math.sin(2* math.pi * t/1440)\n",
    "        npdata[i,5] = math.cos(2* math.pi * t/1440)\n",
    "\n",
    "    ## longitude and latitude\n",
    "    npdata[:,6] = df['Longitude'].values\n",
    "    npdata[:,7] = df['Latitude'].values\n",
    "\n",
    "    ## speed limit\n",
    "    npdata[:,8] = df['Speed_limit'].values\n",
    "\n",
    "    ## Junction Detail\n",
    "    encode = {0: 0, 1:1, 2: 2, 3:3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "    one_hot_code(encode = encode, column = 'Junction_Detail', start = 9, npdata = npdata)\n",
    "\n",
    "    ## Junction Control\n",
    "    junc = df['Junction_Control'].values\n",
    "    i = 0\n",
    "    for j in junc:\n",
    "        if j == -1:\n",
    "            npdata[i,18] = 1\n",
    "        else:\n",
    "            npdata[i, 18+j] = 1\n",
    "        i+=1\n",
    "\n",
    "    ## Light Conditions\n",
    "    encode = {1:0, 4:1, 5:2, 6:3, 7:4}\n",
    "    one_hot_code(encode = encode, column = 'Light_Conditions', start = 23, npdata = npdata)\n",
    "\n",
    "    ## Weather Conditions\n",
    "    encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6, 8:7, 9:8}\n",
    "    one_hot_code(encode = encode, column = 'Weather_Conditions', start = 28, npdata = npdata)\n",
    "\n",
    "    ## Road surface conditions\n",
    "    encode = {1:0, 2: 1, 3: 2, 4: 3, 5:4, 6:5, 7:6}\n",
    "    one_hot_code(encode = encode, column = 'Road_Surface_Conditions', start = 37, npdata = npdata)\n",
    "\n",
    "    ## Special conditions at site\n",
    "    encode = {0:0, 1: 1, 2: 2, 3: 3, 4:4, 5:5, 6:6, 7:7}\n",
    "    one_hot_code(encode = encode, column = 'Special_Conditions_at_Site', start = 44, npdata = npdata)\n",
    "\n",
    "    ## urban or rural\n",
    "    ur = df['Urban_or_Rural_Area'].values\n",
    "    i = 0\n",
    "    for j in ur:\n",
    "        if j==1:\n",
    "            npdata[i, 52] = 1\n",
    "        else:\n",
    "            npdata[i, 52] = -1\n",
    "        i+=1\n",
    "\n",
    "    ## Accident severity, do not encode\n",
    "    npdata[:,53] = df['Accident_Severity'].values\n",
    "\n",
    "    ## Number of KIS\n",
    "    npdata[:,54] = df['Number_of_KIS'].values\n",
    "\n",
    "    np.save('data3/Accidents_'+ year + '_numpy_encoded2.npy', npdata) # save each year's data as numpy array\n",
    "    #implement encoding, wait for normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009: (6638, 55)\n",
      "2010: (6499, 55)\n",
      "2011: (5819, 55)\n",
      "2012: (5607, 55)\n",
      "2013: (5388, 55)\n",
      "2014: (5625, 55)\n",
      "2015: (5537, 55)\n",
      "all data: (41113, 55)\n"
     ]
    }
   ],
   "source": [
    "npdata = np.load('data3/Accidents_2009_numpy_encoded2.npy') # initialise numpy array\n",
    "print ('2009:', npdata.shape)\n",
    "years = ['2010', '2011', '2012', '2013', '2014', '2015']\n",
    "for year in years:\n",
    "    newdata = np.load('data3/Accidents_' + year + '_numpy_encoded2.npy')\n",
    "    print (year+':', newdata.shape)\n",
    "    npdata = np.vstack([npdata,newdata])\n",
    "    \n",
    "print ('all data:', npdata.shape)\n",
    "\n",
    "clm = [0, 1, 2, 3,6,7,8]\n",
    "for c in clm:\n",
    "    norm = npdata[:,c]\n",
    "    norm = (norm - np.mean(norm))/np.std(norm)\n",
    "    npdata[:,c] = norm\n",
    "\n",
    "np.save('data3/Accidents_allyear_numpy_standardised2.npy', npdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npdata = np.load('data3/Accidents_allyear_numpy_standardised2.npy')\n",
    "sum(np.isnan(npdata)*1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accident severity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(29951, 1, 53) (29951,) (5625, 1, 53) (5625,) (41113, 55)\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 50 took 5.095s\n",
      "  training loss:\t\t0.415503\n",
      "  validation loss:\t\t0.405417\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 2 of 50 took 5.269s\n",
      "  training loss:\t\t0.403370\n",
      "  validation loss:\t\t0.400768\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 3 of 50 took 5.042s\n",
      "  training loss:\t\t0.400325\n",
      "  validation loss:\t\t0.400616\n",
      "  validation accuracy:\t\t87.94 %\n",
      "Epoch 4 of 50 took 5.154s\n",
      "  training loss:\t\t0.397930\n",
      "  validation loss:\t\t0.397971\n",
      "  validation accuracy:\t\t87.94 %\n",
      "Epoch 5 of 50 took 5.200s\n",
      "  training loss:\t\t0.396233\n",
      "  validation loss:\t\t0.399154\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 6 of 50 took 5.578s\n",
      "  training loss:\t\t0.395464\n",
      "  validation loss:\t\t0.397706\n",
      "  validation accuracy:\t\t87.94 %\n",
      "Epoch 7 of 50 took 5.659s\n",
      "  training loss:\t\t0.394347\n",
      "  validation loss:\t\t0.397335\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 8 of 50 took 5.593s\n",
      "  training loss:\t\t0.393513\n",
      "  validation loss:\t\t0.397011\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 9 of 50 took 5.301s\n",
      "  training loss:\t\t0.393015\n",
      "  validation loss:\t\t0.396661\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 10 of 50 took 5.223s\n",
      "  training loss:\t\t0.392276\n",
      "  validation loss:\t\t0.397134\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 11 of 50 took 5.235s\n",
      "  training loss:\t\t0.391627\n",
      "  validation loss:\t\t0.396257\n",
      "  validation accuracy:\t\t87.94 %\n",
      "Epoch 12 of 50 took 5.180s\n",
      "  training loss:\t\t0.391103\n",
      "  validation loss:\t\t0.398190\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 13 of 50 took 5.274s\n",
      "  training loss:\t\t0.390571\n",
      "  validation loss:\t\t0.396757\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 14 of 50 took 5.185s\n",
      "  training loss:\t\t0.390265\n",
      "  validation loss:\t\t0.397549\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 15 of 50 took 5.384s\n",
      "  training loss:\t\t0.389515\n",
      "  validation loss:\t\t0.396551\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 16 of 50 took 5.225s\n",
      "  training loss:\t\t0.388970\n",
      "  validation loss:\t\t0.396547\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 17 of 50 took 5.158s\n",
      "  training loss:\t\t0.389187\n",
      "  validation loss:\t\t0.397028\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 18 of 50 took 5.319s\n",
      "  training loss:\t\t0.388234\n",
      "  validation loss:\t\t0.397065\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 19 of 50 took 5.441s\n",
      "  training loss:\t\t0.387311\n",
      "  validation loss:\t\t0.400033\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 20 of 50 took 5.398s\n",
      "  training loss:\t\t0.386855\n",
      "  validation loss:\t\t0.398267\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 21 of 50 took 5.930s\n",
      "  training loss:\t\t0.386647\n",
      "  validation loss:\t\t0.398344\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 22 of 50 took 5.458s\n",
      "  training loss:\t\t0.386029\n",
      "  validation loss:\t\t0.398799\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 23 of 50 took 5.245s\n",
      "  training loss:\t\t0.385747\n",
      "  validation loss:\t\t0.397139\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 24 of 50 took 5.234s\n",
      "  training loss:\t\t0.384957\n",
      "  validation loss:\t\t0.398686\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 25 of 50 took 5.372s\n",
      "  training loss:\t\t0.384302\n",
      "  validation loss:\t\t0.396604\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 26 of 50 took 5.649s\n",
      "  training loss:\t\t0.384009\n",
      "  validation loss:\t\t0.401263\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 27 of 50 took 5.379s\n",
      "  training loss:\t\t0.383095\n",
      "  validation loss:\t\t0.401842\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 28 of 50 took 5.707s\n",
      "  training loss:\t\t0.382873\n",
      "  validation loss:\t\t0.397975\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 29 of 50 took 5.385s\n",
      "  training loss:\t\t0.382466\n",
      "  validation loss:\t\t0.401469\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 30 of 50 took 5.275s\n",
      "  training loss:\t\t0.381697\n",
      "  validation loss:\t\t0.397407\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 31 of 50 took 5.299s\n",
      "  training loss:\t\t0.381545\n",
      "  validation loss:\t\t0.402059\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 32 of 50 took 5.320s\n",
      "  training loss:\t\t0.381144\n",
      "  validation loss:\t\t0.398377\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 33 of 50 took 5.299s\n",
      "  training loss:\t\t0.380377\n",
      "  validation loss:\t\t0.397337\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 34 of 50 took 5.313s\n",
      "  training loss:\t\t0.380004\n",
      "  validation loss:\t\t0.398480\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 35 of 50 took 5.305s\n",
      "  training loss:\t\t0.379342\n",
      "  validation loss:\t\t0.401436\n",
      "  validation accuracy:\t\t87.86 %\n",
      "Epoch 36 of 50 took 5.412s\n",
      "  training loss:\t\t0.378443\n",
      "  validation loss:\t\t0.400677\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 37 of 50 took 5.300s\n",
      "  training loss:\t\t0.378269\n",
      "  validation loss:\t\t0.404368\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 38 of 50 took 5.295s\n",
      "  training loss:\t\t0.377258\n",
      "  validation loss:\t\t0.400487\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 39 of 50 took 5.217s\n",
      "  training loss:\t\t0.377038\n",
      "  validation loss:\t\t0.398994\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 40 of 50 took 5.223s\n",
      "  training loss:\t\t0.376171\n",
      "  validation loss:\t\t0.405336\n",
      "  validation accuracy:\t\t87.94 %\n",
      "Epoch 41 of 50 took 5.204s\n",
      "  training loss:\t\t0.375563\n",
      "  validation loss:\t\t0.402987\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Epoch 42 of 50 took 5.210s\n",
      "  training loss:\t\t0.375017\n",
      "  validation loss:\t\t0.402538\n",
      "  validation accuracy:\t\t87.86 %\n",
      "Epoch 43 of 50 took 5.227s\n",
      "  training loss:\t\t0.374914\n",
      "  validation loss:\t\t0.399931\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 44 of 50 took 5.304s\n",
      "  training loss:\t\t0.373794\n",
      "  validation loss:\t\t0.399910\n",
      "  validation accuracy:\t\t87.88 %\n",
      "Epoch 45 of 50 took 5.225s\n",
      "  training loss:\t\t0.373540\n",
      "  validation loss:\t\t0.403777\n",
      "  validation accuracy:\t\t87.88 %\n",
      "Epoch 46 of 50 took 5.365s\n",
      "  training loss:\t\t0.373206\n",
      "  validation loss:\t\t0.400826\n",
      "  validation accuracy:\t\t87.83 %\n",
      "Epoch 47 of 50 took 5.239s\n",
      "  training loss:\t\t0.371927\n",
      "  validation loss:\t\t0.403487\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 48 of 50 took 5.470s\n",
      "  training loss:\t\t0.371421\n",
      "  validation loss:\t\t0.406131\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 49 of 50 took 5.260s\n",
      "  training loss:\t\t0.370551\n",
      "  validation loss:\t\t0.404825\n",
      "  validation accuracy:\t\t87.92 %\n",
      "Epoch 50 of 50 took 5.294s\n",
      "  training loss:\t\t0.369641\n",
      "  validation loss:\t\t0.404667\n",
      "  validation accuracy:\t\t87.90 %\n",
      "Final results:\n",
      "  test loss:\t\t\t0.435920\n",
      "  test accuracy:\t\t87.16 %\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "\n",
    "# ################## Download and prepare the MNIST dataset ##################\n",
    "# This is just some way of getting the MNIST dataset from an online location\n",
    "# and loading it into numpy arrays. It doesn't involve Lasagne at all.\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    npdata = np.load('data3/Accidents_allyear_numpy_standardised2.npy')\n",
    "    npdata = npdata.astype(np.float32)\n",
    "    X_train, X_val = npdata[0:29951, 0:53].reshape(29951,1,53), npdata[29951:35576, 0:53].reshape(5625,1,53)\n",
    "    y_train, y_val = npdata[0:29951, 53]-1, npdata[29951:35576, 53]-1\n",
    "    x_test, y_test = npdata[35576:41113, 0:53].reshape(5537,1,53), npdata[35576:41113, 53]-1\n",
    "    print (X_train.shape, y_train.shape, X_val.shape, y_val.shape, npdata.shape)\n",
    "    return X_train, y_train, X_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "# ##################### Build the neural network model #######################\n",
    "# This script supports three types of models. For each one, we define a\n",
    "# function that takes a Theano variable representing the input and returns\n",
    "# the output layer of a neural network model built in Lasagne.\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "    # a softmax output layer of 10 units. It applies 20% dropout to the input\n",
    "    # data and 50% dropout to the hidden layers.\n",
    "\n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    # (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
    "    # linking it to the given Theano variable `input_var`, if any:\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None,1,53),\n",
    "                                     input_var=input_var)\n",
    "\n",
    "    # Apply 20% dropout to the input data:\n",
    "    #l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "\n",
    "    # Add a fully-connected layer of 800 units, using the linear rectifier, and\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=300,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # We'll now add dropout of 50%:\n",
    "    #l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    # Another 800-unit layer:\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1, num_units=400,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "#     l_hid3 = lasagne.layers.DenseLayer(\n",
    "#             l_hid2, num_units=500,\n",
    "#             nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid2, num_units=3,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    # Each layer is linked to its incoming layer(s), so we only need to pass\n",
    "    # the output layer to give access to a network in Lasagne:\n",
    "    return l_out\n",
    "\n",
    "\n",
    "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2,\n",
    "                     drop_hidden=.5):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "                network, width, nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 10, nonlinearity=softmax)\n",
    "    return network\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "# ############################## Main program ################################\n",
    "# Everything else will be handled in our main program now. We could pull out\n",
    "# more functions to better separate the code, but it wouldn't make it any\n",
    "# easier to read.\n",
    "\n",
    "def main(model='mlp', num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor3('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    if model == 'mlp':\n",
    "        network = build_mlp(input_var)\n",
    "    elif model.startswith('custom_mlp:'):\n",
    "        depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
    "        network = build_custom_mlp(input_var, int(depth), int(width),\n",
    "                                   float(drop_in), float(drop_hid))\n",
    "    elif model == 'cnn':\n",
    "        network = build_cnn(input_var)\n",
    "    else:\n",
    "        print(\"Unrecognized model type %r.\" % model)\n",
    "        return\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 10, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 10, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if ('--help' in sys.argv) or ('-h' in sys.argv):\n",
    "        print(\"Trains a neural network on MNIST using Lasagne.\")\n",
    "        print(\"Usage: %s [MODEL [EPOCHS]]\" % sys.argv[0])\n",
    "        print()\n",
    "        print(\"MODEL: 'mlp' for a simple Multi-Layer Perceptron (MLP),\")\n",
    "        print(\"       'custom_mlp:DEPTH,WIDTH,DROP_IN,DROP_HID' for an MLP\")\n",
    "        print(\"       with DEPTH hidden layers of WIDTH units, DROP_IN\")\n",
    "        print(\"       input dropout and DROP_HID hidden dropout,\")\n",
    "        print(\"       'cnn' for a simple Convolutional Neural Network (CNN).\")\n",
    "        print(\"EPOCHS: number of training epochs to perform (default: 500)\")\n",
    "    else:\n",
    "        kwargs = {}\n",
    "        if len(sys.argv) > 1:\n",
    "            kwargs['model'] = 'mlp'\n",
    "        if len(sys.argv) > 2:\n",
    "            kwargs['num_epochs'] = 50\n",
    "        main(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    36170\n",
       "2.0     4264\n",
       "1.0      679\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('data3/Accidents_allyear_numpy_standardised2.npy')\n",
    "a = pd.Series(data[:,53])\n",
    "\n",
    "a.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8797703889280762"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36170.0/(36170+4264+679)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     36170\n",
       "1.0      4307\n",
       "2.0       466\n",
       "3.0       102\n",
       "4.0        37\n",
       "6.0        11\n",
       "5.0        11\n",
       "7.0         4\n",
       "8.0         2\n",
       "19.0        1\n",
       "18.0        1\n",
       "9.0         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('data3/Accidents_allyear_numpy_standardised2.npy')\n",
    "a = pd.Series(data[:,54])\n",
    "a.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
